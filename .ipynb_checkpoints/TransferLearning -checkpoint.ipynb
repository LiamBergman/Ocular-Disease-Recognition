{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0af2fd7-206a-42bb-947d-2105b8347f03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from torchvision.transforms import functional as F\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a1ba553-4a61-40eb-90db-5ce1f7e5eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test dataframes\n",
    "train_df = pd.read_csv('data/train_df.csv')\n",
    "test_df = pd.read_csv('data/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26e1b1a3-1cee-45c6-8668-0018e7de880a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Dataset Class\n",
    "class FundusDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.label_mapping = {'D': 0, 'O': 1, 'N': 2}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.df.loc[idx, 'filename'])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label_str = self.df.loc[idx, 'Grouped-Label']\n",
    "        label = self.label_mapping[label_str]\n",
    "        label = torch.tensor(label)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f575fc52-5952-42f4-ac59-d7535b811bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Loading with transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.Lambda(lambda img: F.adjust_brightness(img, brightness_factor=1.2)),\n",
    "    transforms.Lambda(lambda img: F.adjust_contrast(img, contrast_factor=1.2)),\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = FundusDataset(dataframe=train_df, root_dir='data/preprocessed_images', transform=transform)\n",
    "val_dataset = FundusDataset(dataframe=test_df, root_dir='data/preprocessed_images', transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58cdaf42-2521-47fa-9ecf-26251097cb66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet18 and modify output layer\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6411d01c-7fbd-403f-bc8a-fe4e9136e2ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b6a2928-7bce-4bc9-83a5-82e06d399aee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd54287b-2f8a-4bbb-82e9-62e054d623ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Early stopping initialization\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "patience_limit = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6314b45b-0498-4a51-bbbd-34b70db61be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
    "\n",
    "# Initialize metrics\n",
    "train_f1, train_recall, train_accuracy = [], [], []\n",
    "val_f1, val_recall, val_accuracy = [], [], []\n",
    "\n",
    "# Training Loop (for 2 epochs as a demonstration)\n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    true_labels, pred_labels = [], []\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(predicted.cpu().numpy())\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/2], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_f1.append(f1_score(true_labels, pred_labels, average='weighted'))\n",
    "    train_recall.append(recall_score(true_labels, pred_labels, average='weighted'))\n",
    "    train_accuracy.append(accuracy_score(true_labels, pred_labels))\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    true_labels, pred_labels = [], []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            pred_labels.extend(predicted.cpu().numpy())\n",
    "            \n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    val_f1.append(f1_score(true_labels, pred_labels, average='weighted'))\n",
    "    val_recall.append(recall_score(true_labels, pred_labels, average='weighted'))\n",
    "    val_accuracy.append(accuracy_score(true_labels, pred_labels))\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train F1: {train_f1[-1]:.4f}, Train Recall: {train_recall[-1]:.4f}, Train Accuracy: {train_accuracy[-1]:.4f}\")\n",
    "    print(f\"Epoch {epoch+1}, Val F1: {val_f1[-1]:.4f}, Val Recall: {val_recall[-1]:.4f}, Val Accuracy: {val_accuracy[-1]:.4f}\")\n",
    "\n",
    "    # Early Stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience_limit:\n",
    "        print(\"Early stopping.\")\n",
    "        break\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604b30bf-90de-4720-9a13-3f84de0bba7c",
   "metadata": {},
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf65eee-bdc9-48b0-8e15-b8c61711c5f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
    "\n",
    "# Initialize metrics storage\n",
    "train_f1, train_recall, train_accuracy = [], [], []\n",
    "val_f1, val_recall, val_accuracy = [], [], []\n",
    "\n",
    "# Load pre-trained ResNet18 model + higher level API to construct a new output layer\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all layers for initial training\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last layer (classifier) for initial training\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, 3)\n",
    ")\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and Optimizer for the new layer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "# Early stopping initialization\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "patience_limit = 3\n",
    "\n",
    "# Training Loop - Phase 1 (Train only the new layer)\n",
    "for epoch in range(15):  # 2 epochs for demonstration; you can adjust this\n",
    "    model.train()\n",
    "    true_labels, pred_labels = [], []\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "        print(f\"Phase 1 - Epoch [{epoch+1}/2], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Metrics for training\n",
    "    train_f1.append(f1_score(true_labels, pred_labels, average='weighted'))\n",
    "    train_recall.append(recall_score(true_labels, pred_labels, average='weighted'))\n",
    "    train_accuracy.append(accuracy_score(true_labels, pred_labels))\n",
    "\n",
    "    # Validation Loop and metrics\n",
    "    model.eval()\n",
    "    true_labels, pred_labels = [], []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            pred_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_f1.append(f1_score(true_labels, pred_labels, average='weighted'))\n",
    "    val_recall.append(recall_score(true_labels, pred_labels, average='weighted'))\n",
    "    val_accuracy.append(accuracy_score(true_labels, pred_labels))\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train F1: {train_f1[-1]:.4f}, Train Recall: {train_recall[-1]:.4f}, Train Accuracy: {train_accuracy[-1]:.4f}\")\n",
    "    print(f\"Val F1: {val_f1[-1]:.4f}, Val Recall: {val_recall[-1]:.4f}, Val Accuracy: {val_accuracy[-1]:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience_limit:\n",
    "        print(\"Early stopping in Phase 1.\")\n",
    "        break\n",
    "\n",
    "    # Phase 2: Unfreeze some layers (here layer4) and fine-tune\n",
    "    for param in model.layer4.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Update optimizer for Phase 2\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': model.layer4.parameters(), 'lr': 1e-4},\n",
    "        {'params': model.fc.parameters(), 'lr': 1e-3}\n",
    "    ])\n",
    "\n",
    "    # Training Loop - Phase 2\n",
    "for epoch in range(15):  # 2 epochs for demonstration; you can adjust this\n",
    "    model.train()\n",
    "    true_labels, pred_labels = [], []\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "        print(f\"Phase 2 - Epoch [{epoch+1}/2], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Metrics for training\n",
    "    train_f1.append(f1_score(true_labels, pred_labels, average='weighted'))\n",
    "    train_recall.append(recall_score(true_labels, pred_labels, average='weighted'))\n",
    "    train_accuracy.append(accuracy_score(true_labels, pred_labels))\n",
    "\n",
    "    # Validation Loop and metrics\n",
    "    model.eval()\n",
    "    true_labels, pred_labels = [], []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            pred_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_f1.append(f1_score(true_labels, pred_labels, average='weighted'))\n",
    "    val_recall.append(recall_score(true_labels, pred_labels, average='weighted'))\n",
    "    val_accuracy.append(accuracy_score(true_labels, pred_labels))\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train F1: {train_f1[-1]:.4f}, Train Recall: {train_recall[-1]:.4f}, Train Accuracy: {train_accuracy[-1]:.4f}\")\n",
    "    print(f\"Val F1: {val_f1[-1]:.4f}, Val Recall: {val_recall[-1]:.4f}, Val Accuracy: {val_accuracy[-1]:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience_limit:\n",
    "        print(\"Early stopping in Phase 2.\")\n",
    "        break\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acd3c79-054a-41b8-aee5-2e60a1c08f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be0166-1cd0-4e52-af53-1671d3f0d995",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'dropout': 0.3, 'lr': 0.001}\n",
      "Phase 1 - Epoch [1/2], Step [1/160], Loss: 1.1472\n",
      "Phase 1 - Epoch [1/2], Step [2/160], Loss: 1.1355\n",
      "Phase 1 - Epoch [1/2], Step [3/160], Loss: 1.0976\n",
      "Phase 1 - Epoch [1/2], Step [4/160], Loss: 1.1172\n",
      "Phase 1 - Epoch [1/2], Step [5/160], Loss: 1.1179\n",
      "Phase 1 - Epoch [1/2], Step [6/160], Loss: 1.1759\n",
      "Phase 1 - Epoch [1/2], Step [7/160], Loss: 1.1234\n",
      "Phase 1 - Epoch [1/2], Step [8/160], Loss: 1.1411\n",
      "Phase 1 - Epoch [1/2], Step [9/160], Loss: 1.1567\n",
      "Phase 1 - Epoch [1/2], Step [10/160], Loss: 1.0417\n",
      "Phase 1 - Epoch [1/2], Step [11/160], Loss: 1.2018\n",
      "Phase 1 - Epoch [1/2], Step [12/160], Loss: 1.0546\n",
      "Phase 1 - Epoch [1/2], Step [13/160], Loss: 1.0910\n",
      "Phase 1 - Epoch [1/2], Step [14/160], Loss: 1.1575\n",
      "Phase 1 - Epoch [1/2], Step [15/160], Loss: 1.1987\n",
      "Phase 1 - Epoch [1/2], Step [16/160], Loss: 1.1667\n",
      "Phase 1 - Epoch [1/2], Step [17/160], Loss: 1.1980\n",
      "Phase 1 - Epoch [1/2], Step [18/160], Loss: 1.1376\n",
      "Phase 1 - Epoch [1/2], Step [19/160], Loss: 1.1101\n",
      "Phase 1 - Epoch [1/2], Step [20/160], Loss: 1.0932\n",
      "Phase 1 - Epoch [1/2], Step [21/160], Loss: 1.0793\n",
      "Phase 1 - Epoch [1/2], Step [22/160], Loss: 1.1515\n",
      "Phase 1 - Epoch [1/2], Step [23/160], Loss: 1.2457\n",
      "Phase 1 - Epoch [1/2], Step [24/160], Loss: 1.0290\n",
      "Phase 1 - Epoch [1/2], Step [25/160], Loss: 1.1631\n",
      "Phase 1 - Epoch [1/2], Step [26/160], Loss: 1.1201\n",
      "Phase 1 - Epoch [1/2], Step [27/160], Loss: 1.1496\n",
      "Phase 1 - Epoch [1/2], Step [28/160], Loss: 1.0965\n",
      "Phase 1 - Epoch [1/2], Step [29/160], Loss: 1.1835\n",
      "Phase 1 - Epoch [1/2], Step [30/160], Loss: 1.0411\n",
      "Phase 1 - Epoch [1/2], Step [31/160], Loss: 1.0635\n",
      "Phase 1 - Epoch [1/2], Step [32/160], Loss: 1.0092\n",
      "Phase 1 - Epoch [1/2], Step [33/160], Loss: 0.9687\n",
      "Phase 1 - Epoch [1/2], Step [34/160], Loss: 1.2206\n",
      "Phase 1 - Epoch [1/2], Step [35/160], Loss: 1.1041\n",
      "Phase 1 - Epoch [1/2], Step [36/160], Loss: 1.0978\n",
      "Phase 1 - Epoch [1/2], Step [37/160], Loss: 1.0649\n",
      "Phase 1 - Epoch [1/2], Step [38/160], Loss: 1.1934\n",
      "Phase 1 - Epoch [1/2], Step [39/160], Loss: 1.0895\n",
      "Phase 1 - Epoch [1/2], Step [40/160], Loss: 1.1958\n",
      "Phase 1 - Epoch [1/2], Step [41/160], Loss: 1.1215\n",
      "Phase 1 - Epoch [1/2], Step [42/160], Loss: 1.1636\n",
      "Phase 1 - Epoch [1/2], Step [43/160], Loss: 1.1735\n",
      "Phase 1 - Epoch [1/2], Step [44/160], Loss: 1.0971\n",
      "Phase 1 - Epoch [1/2], Step [45/160], Loss: 1.0852\n",
      "Phase 1 - Epoch [1/2], Step [46/160], Loss: 1.0397\n",
      "Phase 1 - Epoch [1/2], Step [47/160], Loss: 1.1381\n",
      "Phase 1 - Epoch [1/2], Step [48/160], Loss: 1.1264\n",
      "Phase 1 - Epoch [1/2], Step [49/160], Loss: 1.2361\n",
      "Phase 1 - Epoch [1/2], Step [50/160], Loss: 1.0818\n",
      "Phase 1 - Epoch [1/2], Step [51/160], Loss: 1.0694\n",
      "Phase 1 - Epoch [1/2], Step [52/160], Loss: 1.1521\n",
      "Phase 1 - Epoch [1/2], Step [53/160], Loss: 1.0618\n",
      "Phase 1 - Epoch [1/2], Step [54/160], Loss: 1.0267\n",
      "Phase 1 - Epoch [1/2], Step [55/160], Loss: 1.0825\n",
      "Phase 1 - Epoch [1/2], Step [56/160], Loss: 1.0586\n",
      "Phase 1 - Epoch [1/2], Step [57/160], Loss: 1.0642\n",
      "Phase 1 - Epoch [1/2], Step [58/160], Loss: 1.1160\n",
      "Phase 1 - Epoch [1/2], Step [59/160], Loss: 1.0454\n",
      "Phase 1 - Epoch [1/2], Step [60/160], Loss: 1.0112\n",
      "Phase 1 - Epoch [1/2], Step [61/160], Loss: 1.0376\n",
      "Phase 1 - Epoch [1/2], Step [62/160], Loss: 1.0500\n",
      "Phase 1 - Epoch [1/2], Step [63/160], Loss: 1.0130\n",
      "Phase 1 - Epoch [1/2], Step [64/160], Loss: 1.1337\n",
      "Phase 1 - Epoch [1/2], Step [65/160], Loss: 1.0220\n",
      "Phase 1 - Epoch [1/2], Step [66/160], Loss: 1.0565\n",
      "Phase 1 - Epoch [1/2], Step [67/160], Loss: 1.0761\n",
      "Phase 1 - Epoch [1/2], Step [68/160], Loss: 1.1396\n",
      "Phase 1 - Epoch [1/2], Step [69/160], Loss: 1.1177\n",
      "Phase 1 - Epoch [1/2], Step [70/160], Loss: 1.0145\n",
      "Phase 1 - Epoch [1/2], Step [71/160], Loss: 1.0457\n",
      "Phase 1 - Epoch [1/2], Step [72/160], Loss: 1.1071\n",
      "Phase 1 - Epoch [1/2], Step [73/160], Loss: 1.0657\n",
      "Phase 1 - Epoch [1/2], Step [74/160], Loss: 1.0820\n",
      "Phase 1 - Epoch [1/2], Step [75/160], Loss: 1.0668\n",
      "Phase 1 - Epoch [1/2], Step [76/160], Loss: 1.0214\n",
      "Phase 1 - Epoch [1/2], Step [77/160], Loss: 1.1402\n",
      "Phase 1 - Epoch [1/2], Step [78/160], Loss: 1.1183\n",
      "Phase 1 - Epoch [1/2], Step [79/160], Loss: 1.1257\n",
      "Phase 1 - Epoch [1/2], Step [80/160], Loss: 1.1061\n",
      "Phase 1 - Epoch [1/2], Step [81/160], Loss: 1.0598\n",
      "Phase 1 - Epoch [1/2], Step [82/160], Loss: 1.0493\n",
      "Phase 1 - Epoch [1/2], Step [83/160], Loss: 1.0773\n",
      "Phase 1 - Epoch [1/2], Step [84/160], Loss: 1.1723\n",
      "Phase 1 - Epoch [1/2], Step [85/160], Loss: 1.0137\n",
      "Phase 1 - Epoch [1/2], Step [86/160], Loss: 1.1081\n",
      "Phase 1 - Epoch [1/2], Step [87/160], Loss: 1.0602\n",
      "Phase 1 - Epoch [1/2], Step [88/160], Loss: 1.0745\n",
      "Phase 1 - Epoch [1/2], Step [89/160], Loss: 1.1288\n",
      "Phase 1 - Epoch [1/2], Step [90/160], Loss: 1.0234\n",
      "Phase 1 - Epoch [1/2], Step [91/160], Loss: 1.0310\n",
      "Phase 1 - Epoch [1/2], Step [92/160], Loss: 1.0708\n",
      "Phase 1 - Epoch [1/2], Step [93/160], Loss: 1.1303\n",
      "Phase 1 - Epoch [1/2], Step [94/160], Loss: 1.0754\n",
      "Phase 1 - Epoch [1/2], Step [95/160], Loss: 1.1417\n",
      "Phase 1 - Epoch [1/2], Step [96/160], Loss: 1.0505\n",
      "Phase 1 - Epoch [1/2], Step [97/160], Loss: 1.0502\n",
      "Phase 1 - Epoch [1/2], Step [98/160], Loss: 1.0116\n",
      "Phase 1 - Epoch [1/2], Step [99/160], Loss: 1.0977\n",
      "Phase 1 - Epoch [1/2], Step [100/160], Loss: 1.0378\n",
      "Phase 1 - Epoch [1/2], Step [101/160], Loss: 1.0289\n",
      "Phase 1 - Epoch [1/2], Step [102/160], Loss: 1.1319\n",
      "Phase 1 - Epoch [1/2], Step [103/160], Loss: 1.0847\n",
      "Phase 1 - Epoch [1/2], Step [104/160], Loss: 1.0177\n",
      "Phase 1 - Epoch [1/2], Step [105/160], Loss: 1.1131\n",
      "Phase 1 - Epoch [1/2], Step [106/160], Loss: 1.1631\n",
      "Phase 1 - Epoch [1/2], Step [107/160], Loss: 1.1121\n",
      "Phase 1 - Epoch [1/2], Step [108/160], Loss: 1.0181\n",
      "Phase 1 - Epoch [1/2], Step [109/160], Loss: 1.0282\n",
      "Phase 1 - Epoch [1/2], Step [110/160], Loss: 1.0501\n",
      "Phase 1 - Epoch [1/2], Step [111/160], Loss: 1.0147\n",
      "Phase 1 - Epoch [1/2], Step [112/160], Loss: 1.0363\n",
      "Phase 1 - Epoch [1/2], Step [113/160], Loss: 1.1351\n",
      "Phase 1 - Epoch [1/2], Step [114/160], Loss: 1.0037\n",
      "Phase 1 - Epoch [1/2], Step [115/160], Loss: 1.0179\n",
      "Phase 1 - Epoch [1/2], Step [116/160], Loss: 1.0979\n",
      "Phase 1 - Epoch [1/2], Step [117/160], Loss: 1.1286\n",
      "Phase 1 - Epoch [1/2], Step [118/160], Loss: 1.1180\n",
      "Phase 1 - Epoch [1/2], Step [119/160], Loss: 1.0755\n",
      "Phase 1 - Epoch [1/2], Step [120/160], Loss: 1.0252\n",
      "Phase 1 - Epoch [1/2], Step [121/160], Loss: 1.1173\n",
      "Phase 1 - Epoch [1/2], Step [122/160], Loss: 1.0602\n",
      "Phase 1 - Epoch [1/2], Step [123/160], Loss: 0.9919\n",
      "Phase 1 - Epoch [1/2], Step [124/160], Loss: 1.0118\n",
      "Phase 1 - Epoch [1/2], Step [125/160], Loss: 1.1093\n",
      "Phase 1 - Epoch [1/2], Step [126/160], Loss: 1.0257\n",
      "Phase 1 - Epoch [1/2], Step [127/160], Loss: 1.1507\n",
      "Phase 1 - Epoch [1/2], Step [128/160], Loss: 0.9065\n",
      "Phase 1 - Epoch [1/2], Step [129/160], Loss: 1.0983\n",
      "Phase 1 - Epoch [1/2], Step [130/160], Loss: 0.9890\n",
      "Phase 1 - Epoch [1/2], Step [131/160], Loss: 1.0423\n",
      "Phase 1 - Epoch [1/2], Step [132/160], Loss: 1.0147\n",
      "Phase 1 - Epoch [1/2], Step [133/160], Loss: 1.0599\n",
      "Phase 1 - Epoch [1/2], Step [134/160], Loss: 0.9292\n",
      "Phase 1 - Epoch [1/2], Step [135/160], Loss: 1.0679\n",
      "Phase 1 - Epoch [1/2], Step [136/160], Loss: 0.8632\n",
      "Phase 1 - Epoch [1/2], Step [137/160], Loss: 1.0435\n",
      "Phase 1 - Epoch [1/2], Step [138/160], Loss: 0.9844\n",
      "Phase 1 - Epoch [1/2], Step [139/160], Loss: 0.9422\n",
      "Phase 1 - Epoch [1/2], Step [140/160], Loss: 1.0032\n",
      "Phase 1 - Epoch [1/2], Step [141/160], Loss: 1.0144\n",
      "Phase 1 - Epoch [1/2], Step [142/160], Loss: 1.0145\n",
      "Phase 1 - Epoch [1/2], Step [143/160], Loss: 0.9337\n",
      "Phase 1 - Epoch [1/2], Step [144/160], Loss: 1.0164\n",
      "Phase 1 - Epoch [1/2], Step [145/160], Loss: 1.0930\n",
      "Phase 1 - Epoch [1/2], Step [146/160], Loss: 1.0509\n",
      "Phase 1 - Epoch [1/2], Step [147/160], Loss: 0.9494\n",
      "Phase 1 - Epoch [1/2], Step [148/160], Loss: 1.0310\n",
      "Phase 1 - Epoch [1/2], Step [149/160], Loss: 1.0496\n",
      "Phase 1 - Epoch [1/2], Step [150/160], Loss: 1.0265\n",
      "Phase 1 - Epoch [1/2], Step [151/160], Loss: 1.1074\n",
      "Phase 1 - Epoch [1/2], Step [152/160], Loss: 0.9547\n",
      "Phase 1 - Epoch [1/2], Step [153/160], Loss: 0.9701\n",
      "Phase 1 - Epoch [1/2], Step [154/160], Loss: 1.0285\n",
      "Phase 1 - Epoch [1/2], Step [155/160], Loss: 1.0429\n",
      "Phase 1 - Epoch [1/2], Step [156/160], Loss: 0.9768\n",
      "Phase 1 - Epoch [1/2], Step [157/160], Loss: 1.1188\n",
      "Phase 1 - Epoch [1/2], Step [158/160], Loss: 1.0561\n",
      "Phase 1 - Epoch [1/2], Step [159/160], Loss: 1.1218\n",
      "Phase 1 - Epoch [1/2], Step [160/160], Loss: 1.0087\n",
      "Validation Loss: 1.0272\n",
      "Train F1: 0.4157, Train Recall: 0.4170, Train Accuracy: 0.4170\n",
      "Val F1: 0.4657, Val Recall: 0.4707, Val Accuracy: 0.4707\n",
      "Best recall so far: 0.47068021892103207, Best parameters so far: {'dropout': 0.3, 'lr': 0.001}\n",
      "Phase 1 - Epoch [2/2], Step [1/160], Loss: 1.0342\n",
      "Phase 1 - Epoch [2/2], Step [2/160], Loss: 1.1180\n",
      "Phase 1 - Epoch [2/2], Step [3/160], Loss: 1.0039\n",
      "Phase 1 - Epoch [2/2], Step [4/160], Loss: 1.0806\n",
      "Phase 1 - Epoch [2/2], Step [5/160], Loss: 0.9283\n",
      "Phase 1 - Epoch [2/2], Step [6/160], Loss: 0.9939\n",
      "Phase 1 - Epoch [2/2], Step [7/160], Loss: 1.0360\n",
      "Phase 1 - Epoch [2/2], Step [8/160], Loss: 1.0209\n",
      "Phase 1 - Epoch [2/2], Step [9/160], Loss: 1.0585\n",
      "Phase 1 - Epoch [2/2], Step [10/160], Loss: 1.0929\n",
      "Phase 1 - Epoch [2/2], Step [11/160], Loss: 1.1471\n",
      "Phase 1 - Epoch [2/2], Step [12/160], Loss: 1.1079\n",
      "Phase 1 - Epoch [2/2], Step [13/160], Loss: 1.1346\n",
      "Phase 1 - Epoch [2/2], Step [14/160], Loss: 1.0559\n",
      "Phase 1 - Epoch [2/2], Step [15/160], Loss: 0.9811\n",
      "Phase 1 - Epoch [2/2], Step [16/160], Loss: 1.0116\n",
      "Phase 1 - Epoch [2/2], Step [17/160], Loss: 1.0425\n",
      "Phase 1 - Epoch [2/2], Step [18/160], Loss: 1.0380\n",
      "Phase 1 - Epoch [2/2], Step [19/160], Loss: 1.0775\n",
      "Phase 1 - Epoch [2/2], Step [20/160], Loss: 1.0499\n",
      "Phase 1 - Epoch [2/2], Step [21/160], Loss: 1.0146\n",
      "Phase 1 - Epoch [2/2], Step [22/160], Loss: 1.0153\n",
      "Phase 1 - Epoch [2/2], Step [23/160], Loss: 1.0690\n",
      "Phase 1 - Epoch [2/2], Step [24/160], Loss: 1.0643\n",
      "Phase 1 - Epoch [2/2], Step [25/160], Loss: 1.0138\n",
      "Phase 1 - Epoch [2/2], Step [26/160], Loss: 0.8918\n",
      "Phase 1 - Epoch [2/2], Step [27/160], Loss: 1.1119\n",
      "Phase 1 - Epoch [2/2], Step [28/160], Loss: 1.0573\n",
      "Phase 1 - Epoch [2/2], Step [29/160], Loss: 1.0477\n",
      "Phase 1 - Epoch [2/2], Step [30/160], Loss: 1.0274\n",
      "Phase 1 - Epoch [2/2], Step [31/160], Loss: 1.0733\n",
      "Phase 1 - Epoch [2/2], Step [32/160], Loss: 0.9569\n",
      "Phase 1 - Epoch [2/2], Step [33/160], Loss: 0.9471\n",
      "Phase 1 - Epoch [2/2], Step [34/160], Loss: 1.0999\n",
      "Phase 1 - Epoch [2/2], Step [35/160], Loss: 0.9259\n",
      "Phase 1 - Epoch [2/2], Step [36/160], Loss: 0.9608\n",
      "Phase 1 - Epoch [2/2], Step [37/160], Loss: 1.0562\n",
      "Phase 1 - Epoch [2/2], Step [38/160], Loss: 0.9819\n",
      "Phase 1 - Epoch [2/2], Step [39/160], Loss: 1.0133\n",
      "Phase 1 - Epoch [2/2], Step [40/160], Loss: 1.0120\n",
      "Phase 1 - Epoch [2/2], Step [41/160], Loss: 0.9974\n",
      "Phase 1 - Epoch [2/2], Step [42/160], Loss: 1.0068\n",
      "Phase 1 - Epoch [2/2], Step [43/160], Loss: 1.1923\n",
      "Phase 1 - Epoch [2/2], Step [44/160], Loss: 1.0874\n",
      "Phase 1 - Epoch [2/2], Step [45/160], Loss: 1.0137\n",
      "Phase 1 - Epoch [2/2], Step [46/160], Loss: 1.0490\n",
      "Phase 1 - Epoch [2/2], Step [47/160], Loss: 1.0846\n",
      "Phase 1 - Epoch [2/2], Step [48/160], Loss: 0.9916\n",
      "Phase 1 - Epoch [2/2], Step [49/160], Loss: 0.9079\n",
      "Phase 1 - Epoch [2/2], Step [50/160], Loss: 1.0586\n",
      "Phase 1 - Epoch [2/2], Step [51/160], Loss: 0.9882\n",
      "Phase 1 - Epoch [2/2], Step [52/160], Loss: 0.9892\n",
      "Phase 1 - Epoch [2/2], Step [53/160], Loss: 0.9202\n",
      "Phase 1 - Epoch [2/2], Step [54/160], Loss: 1.0880\n",
      "Phase 1 - Epoch [2/2], Step [55/160], Loss: 1.1632\n",
      "Phase 1 - Epoch [2/2], Step [56/160], Loss: 1.0419\n",
      "Phase 1 - Epoch [2/2], Step [57/160], Loss: 0.9459\n",
      "Phase 1 - Epoch [2/2], Step [58/160], Loss: 0.8390\n",
      "Phase 1 - Epoch [2/2], Step [59/160], Loss: 0.9736\n",
      "Phase 1 - Epoch [2/2], Step [60/160], Loss: 0.9568\n",
      "Phase 1 - Epoch [2/2], Step [61/160], Loss: 0.9804\n",
      "Phase 1 - Epoch [2/2], Step [62/160], Loss: 1.1391\n",
      "Phase 1 - Epoch [2/2], Step [63/160], Loss: 1.1292\n",
      "Phase 1 - Epoch [2/2], Step [64/160], Loss: 0.9977\n",
      "Phase 1 - Epoch [2/2], Step [65/160], Loss: 0.9538\n",
      "Phase 1 - Epoch [2/2], Step [66/160], Loss: 0.9933\n",
      "Phase 1 - Epoch [2/2], Step [67/160], Loss: 1.0400\n",
      "Phase 1 - Epoch [2/2], Step [68/160], Loss: 0.9392\n",
      "Phase 1 - Epoch [2/2], Step [69/160], Loss: 1.0189\n",
      "Phase 1 - Epoch [2/2], Step [70/160], Loss: 1.0723\n",
      "Phase 1 - Epoch [2/2], Step [71/160], Loss: 0.9360\n",
      "Phase 1 - Epoch [2/2], Step [72/160], Loss: 0.9768\n",
      "Phase 1 - Epoch [2/2], Step [73/160], Loss: 1.0023\n",
      "Phase 1 - Epoch [2/2], Step [74/160], Loss: 0.8925\n",
      "Phase 1 - Epoch [2/2], Step [75/160], Loss: 1.0167\n",
      "Phase 1 - Epoch [2/2], Step [76/160], Loss: 0.9465\n",
      "Phase 1 - Epoch [2/2], Step [77/160], Loss: 0.9523\n",
      "Phase 1 - Epoch [2/2], Step [78/160], Loss: 1.0160\n",
      "Phase 1 - Epoch [2/2], Step [79/160], Loss: 0.9154\n",
      "Phase 1 - Epoch [2/2], Step [80/160], Loss: 1.1400\n",
      "Phase 1 - Epoch [2/2], Step [81/160], Loss: 1.2906\n",
      "Phase 1 - Epoch [2/2], Step [82/160], Loss: 0.9903\n",
      "Phase 1 - Epoch [2/2], Step [83/160], Loss: 1.0036\n",
      "Phase 1 - Epoch [2/2], Step [84/160], Loss: 0.9735\n",
      "Phase 1 - Epoch [2/2], Step [85/160], Loss: 1.0357\n",
      "Phase 1 - Epoch [2/2], Step [86/160], Loss: 0.8910\n",
      "Phase 1 - Epoch [2/2], Step [87/160], Loss: 0.9953\n",
      "Phase 1 - Epoch [2/2], Step [88/160], Loss: 1.0220\n",
      "Phase 1 - Epoch [2/2], Step [89/160], Loss: 0.9496\n",
      "Phase 1 - Epoch [2/2], Step [90/160], Loss: 0.9446\n",
      "Phase 1 - Epoch [2/2], Step [91/160], Loss: 0.9078\n",
      "Phase 1 - Epoch [2/2], Step [92/160], Loss: 0.9243\n",
      "Phase 1 - Epoch [2/2], Step [93/160], Loss: 0.9093\n",
      "Phase 1 - Epoch [2/2], Step [94/160], Loss: 1.0575\n",
      "Phase 1 - Epoch [2/2], Step [95/160], Loss: 0.8634\n",
      "Phase 1 - Epoch [2/2], Step [96/160], Loss: 0.9816\n",
      "Phase 1 - Epoch [2/2], Step [97/160], Loss: 1.0663\n",
      "Phase 1 - Epoch [2/2], Step [98/160], Loss: 0.9977\n",
      "Phase 1 - Epoch [2/2], Step [99/160], Loss: 0.9144\n",
      "Phase 1 - Epoch [2/2], Step [100/160], Loss: 0.8784\n",
      "Phase 1 - Epoch [2/2], Step [101/160], Loss: 0.9041\n",
      "Phase 1 - Epoch [2/2], Step [102/160], Loss: 0.9324\n",
      "Phase 1 - Epoch [2/2], Step [103/160], Loss: 0.8871\n",
      "Phase 1 - Epoch [2/2], Step [104/160], Loss: 1.1239\n",
      "Phase 1 - Epoch [2/2], Step [105/160], Loss: 1.0497\n",
      "Phase 1 - Epoch [2/2], Step [106/160], Loss: 0.8700\n",
      "Phase 1 - Epoch [2/2], Step [107/160], Loss: 1.1378\n",
      "Phase 1 - Epoch [2/2], Step [108/160], Loss: 0.9097\n",
      "Phase 1 - Epoch [2/2], Step [109/160], Loss: 1.0440\n",
      "Phase 1 - Epoch [2/2], Step [110/160], Loss: 0.9733\n",
      "Phase 1 - Epoch [2/2], Step [111/160], Loss: 1.0076\n",
      "Phase 1 - Epoch [2/2], Step [112/160], Loss: 0.8565\n",
      "Phase 1 - Epoch [2/2], Step [113/160], Loss: 1.0043\n",
      "Phase 1 - Epoch [2/2], Step [114/160], Loss: 0.8533\n",
      "Phase 1 - Epoch [2/2], Step [115/160], Loss: 1.0097\n",
      "Phase 1 - Epoch [2/2], Step [116/160], Loss: 0.9939\n",
      "Phase 1 - Epoch [2/2], Step [117/160], Loss: 0.9508\n",
      "Phase 1 - Epoch [2/2], Step [118/160], Loss: 0.9274\n",
      "Phase 1 - Epoch [2/2], Step [119/160], Loss: 0.9718\n",
      "Phase 1 - Epoch [2/2], Step [120/160], Loss: 0.9235\n",
      "Phase 1 - Epoch [2/2], Step [121/160], Loss: 1.1482\n",
      "Phase 1 - Epoch [2/2], Step [122/160], Loss: 0.8720\n",
      "Phase 1 - Epoch [2/2], Step [123/160], Loss: 0.9826\n",
      "Phase 1 - Epoch [2/2], Step [124/160], Loss: 0.9447\n",
      "Phase 1 - Epoch [2/2], Step [125/160], Loss: 0.8389\n",
      "Phase 1 - Epoch [2/2], Step [126/160], Loss: 1.0118\n",
      "Phase 1 - Epoch [2/2], Step [127/160], Loss: 0.9414\n",
      "Phase 1 - Epoch [2/2], Step [128/160], Loss: 1.0041\n",
      "Phase 1 - Epoch [2/2], Step [129/160], Loss: 0.9136\n",
      "Phase 1 - Epoch [2/2], Step [130/160], Loss: 1.0339\n",
      "Phase 1 - Epoch [2/2], Step [131/160], Loss: 1.1474\n",
      "Phase 1 - Epoch [2/2], Step [132/160], Loss: 1.0058\n",
      "Phase 1 - Epoch [2/2], Step [133/160], Loss: 0.9023\n",
      "Phase 1 - Epoch [2/2], Step [134/160], Loss: 0.9242\n",
      "Phase 1 - Epoch [2/2], Step [135/160], Loss: 0.9254\n",
      "Phase 1 - Epoch [2/2], Step [136/160], Loss: 0.9831\n",
      "Phase 1 - Epoch [2/2], Step [137/160], Loss: 0.9435\n",
      "Phase 1 - Epoch [2/2], Step [138/160], Loss: 0.8196\n",
      "Phase 1 - Epoch [2/2], Step [139/160], Loss: 0.9036\n",
      "Phase 1 - Epoch [2/2], Step [140/160], Loss: 0.9487\n",
      "Phase 1 - Epoch [2/2], Step [141/160], Loss: 0.8441\n",
      "Phase 1 - Epoch [2/2], Step [142/160], Loss: 1.3496\n",
      "Phase 1 - Epoch [2/2], Step [143/160], Loss: 1.2457\n",
      "Phase 1 - Epoch [2/2], Step [144/160], Loss: 0.9563\n",
      "Phase 1 - Epoch [2/2], Step [145/160], Loss: 1.1237\n",
      "Phase 1 - Epoch [2/2], Step [146/160], Loss: 1.0980\n",
      "Phase 1 - Epoch [2/2], Step [147/160], Loss: 0.9257\n",
      "Phase 1 - Epoch [2/2], Step [148/160], Loss: 1.0897\n",
      "Phase 1 - Epoch [2/2], Step [149/160], Loss: 1.1494\n",
      "Phase 1 - Epoch [2/2], Step [150/160], Loss: 0.8534\n",
      "Phase 1 - Epoch [2/2], Step [151/160], Loss: 0.9481\n",
      "Phase 1 - Epoch [2/2], Step [152/160], Loss: 0.9493\n",
      "Phase 1 - Epoch [2/2], Step [153/160], Loss: 0.9414\n",
      "Phase 1 - Epoch [2/2], Step [154/160], Loss: 0.9614\n",
      "Phase 1 - Epoch [2/2], Step [155/160], Loss: 0.8717\n",
      "Phase 1 - Epoch [2/2], Step [156/160], Loss: 0.8659\n",
      "Phase 1 - Epoch [2/2], Step [157/160], Loss: 0.8957\n",
      "Phase 1 - Epoch [2/2], Step [158/160], Loss: 0.9054\n",
      "Phase 1 - Epoch [2/2], Step [159/160], Loss: 1.0090\n",
      "Phase 1 - Epoch [2/2], Step [160/160], Loss: 0.8431\n",
      "Validation Loss: 0.9405\n",
      "Train F1: 0.5034, Train Recall: 0.5032, Train Accuracy: 0.5032\n",
      "Val F1: 0.5291, Val Recall: 0.5309, Val Accuracy: 0.5309\n",
      "Best recall so far: 0.5308835027365129, Best parameters so far: {'dropout': 0.3, 'lr': 0.001}\n",
      "Phase 1 - Epoch [3/2], Step [1/160], Loss: 0.8719\n",
      "Phase 1 - Epoch [3/2], Step [2/160], Loss: 0.9029\n",
      "Phase 1 - Epoch [3/2], Step [3/160], Loss: 0.9811\n",
      "Phase 1 - Epoch [3/2], Step [4/160], Loss: 0.9390\n",
      "Phase 1 - Epoch [3/2], Step [5/160], Loss: 1.1801\n",
      "Phase 1 - Epoch [3/2], Step [6/160], Loss: 0.9986\n",
      "Phase 1 - Epoch [3/2], Step [7/160], Loss: 0.9801\n",
      "Phase 1 - Epoch [3/2], Step [8/160], Loss: 1.0068\n",
      "Phase 1 - Epoch [3/2], Step [9/160], Loss: 0.8731\n",
      "Phase 1 - Epoch [3/2], Step [10/160], Loss: 0.7695\n",
      "Phase 1 - Epoch [3/2], Step [11/160], Loss: 0.9742\n",
      "Phase 1 - Epoch [3/2], Step [12/160], Loss: 0.8689\n",
      "Phase 1 - Epoch [3/2], Step [13/160], Loss: 1.0197\n",
      "Phase 1 - Epoch [3/2], Step [14/160], Loss: 0.9316\n",
      "Phase 1 - Epoch [3/2], Step [15/160], Loss: 0.9857\n",
      "Phase 1 - Epoch [3/2], Step [16/160], Loss: 1.0220\n",
      "Phase 1 - Epoch [3/2], Step [17/160], Loss: 0.8636\n",
      "Phase 1 - Epoch [3/2], Step [18/160], Loss: 0.9360\n",
      "Phase 1 - Epoch [3/2], Step [19/160], Loss: 0.9678\n",
      "Phase 1 - Epoch [3/2], Step [20/160], Loss: 0.8903\n",
      "Phase 1 - Epoch [3/2], Step [21/160], Loss: 0.7895\n",
      "Phase 1 - Epoch [3/2], Step [22/160], Loss: 0.8948\n",
      "Phase 1 - Epoch [3/2], Step [23/160], Loss: 0.9783\n",
      "Phase 1 - Epoch [3/2], Step [24/160], Loss: 0.9113\n",
      "Phase 1 - Epoch [3/2], Step [25/160], Loss: 1.0145\n",
      "Phase 1 - Epoch [3/2], Step [26/160], Loss: 0.8460\n",
      "Phase 1 - Epoch [3/2], Step [27/160], Loss: 0.9344\n",
      "Phase 1 - Epoch [3/2], Step [28/160], Loss: 0.7565\n",
      "Phase 1 - Epoch [3/2], Step [29/160], Loss: 0.8694\n",
      "Phase 1 - Epoch [3/2], Step [30/160], Loss: 0.8464\n",
      "Phase 1 - Epoch [3/2], Step [31/160], Loss: 1.0506\n",
      "Phase 1 - Epoch [3/2], Step [32/160], Loss: 0.8601\n",
      "Phase 1 - Epoch [3/2], Step [33/160], Loss: 0.8998\n",
      "Phase 1 - Epoch [3/2], Step [34/160], Loss: 1.0004\n",
      "Phase 1 - Epoch [3/2], Step [35/160], Loss: 0.7980\n",
      "Phase 1 - Epoch [3/2], Step [36/160], Loss: 0.8677\n",
      "Phase 1 - Epoch [3/2], Step [37/160], Loss: 1.0734\n",
      "Phase 1 - Epoch [3/2], Step [38/160], Loss: 0.8680\n",
      "Phase 1 - Epoch [3/2], Step [39/160], Loss: 0.9178\n",
      "Phase 1 - Epoch [3/2], Step [40/160], Loss: 1.0797\n",
      "Phase 1 - Epoch [3/2], Step [41/160], Loss: 1.0427\n",
      "Phase 1 - Epoch [3/2], Step [42/160], Loss: 0.8473\n",
      "Phase 1 - Epoch [3/2], Step [43/160], Loss: 0.8386\n",
      "Phase 1 - Epoch [3/2], Step [44/160], Loss: 0.9020\n",
      "Phase 1 - Epoch [3/2], Step [45/160], Loss: 1.0099\n",
      "Phase 1 - Epoch [3/2], Step [46/160], Loss: 0.8245\n",
      "Phase 1 - Epoch [3/2], Step [47/160], Loss: 0.9388\n",
      "Phase 1 - Epoch [3/2], Step [48/160], Loss: 0.9108\n",
      "Phase 1 - Epoch [3/2], Step [49/160], Loss: 1.0341\n",
      "Phase 1 - Epoch [3/2], Step [50/160], Loss: 1.0240\n",
      "Phase 1 - Epoch [3/2], Step [51/160], Loss: 0.9661\n",
      "Phase 1 - Epoch [3/2], Step [52/160], Loss: 0.9148\n",
      "Phase 1 - Epoch [3/2], Step [53/160], Loss: 0.9537\n",
      "Phase 1 - Epoch [3/2], Step [54/160], Loss: 0.9624\n",
      "Phase 1 - Epoch [3/2], Step [55/160], Loss: 0.7789\n",
      "Phase 1 - Epoch [3/2], Step [56/160], Loss: 0.9830\n",
      "Phase 1 - Epoch [3/2], Step [57/160], Loss: 0.8729\n",
      "Phase 1 - Epoch [3/2], Step [58/160], Loss: 1.0346\n",
      "Phase 1 - Epoch [3/2], Step [59/160], Loss: 0.9219\n",
      "Phase 1 - Epoch [3/2], Step [60/160], Loss: 0.8867\n",
      "Phase 1 - Epoch [3/2], Step [61/160], Loss: 1.0131\n",
      "Phase 1 - Epoch [3/2], Step [62/160], Loss: 0.7851\n",
      "Phase 1 - Epoch [3/2], Step [63/160], Loss: 1.1185\n",
      "Phase 1 - Epoch [3/2], Step [64/160], Loss: 0.9172\n",
      "Phase 1 - Epoch [3/2], Step [65/160], Loss: 1.1017\n",
      "Phase 1 - Epoch [3/2], Step [66/160], Loss: 0.8540\n",
      "Phase 1 - Epoch [3/2], Step [67/160], Loss: 0.9519\n",
      "Phase 1 - Epoch [3/2], Step [68/160], Loss: 0.9290\n",
      "Phase 1 - Epoch [3/2], Step [69/160], Loss: 0.8638\n",
      "Phase 1 - Epoch [3/2], Step [70/160], Loss: 0.8245\n",
      "Phase 1 - Epoch [3/2], Step [71/160], Loss: 0.8160\n",
      "Phase 1 - Epoch [3/2], Step [72/160], Loss: 0.8049\n",
      "Phase 1 - Epoch [3/2], Step [73/160], Loss: 0.7361\n",
      "Phase 1 - Epoch [3/2], Step [74/160], Loss: 1.1016\n",
      "Phase 1 - Epoch [3/2], Step [75/160], Loss: 0.8858\n",
      "Phase 1 - Epoch [3/2], Step [76/160], Loss: 0.8896\n",
      "Phase 1 - Epoch [3/2], Step [77/160], Loss: 0.8909\n",
      "Phase 1 - Epoch [3/2], Step [78/160], Loss: 0.9745\n",
      "Phase 1 - Epoch [3/2], Step [79/160], Loss: 0.9941\n",
      "Phase 1 - Epoch [3/2], Step [80/160], Loss: 0.9582\n",
      "Phase 1 - Epoch [3/2], Step [81/160], Loss: 0.8718\n",
      "Phase 1 - Epoch [3/2], Step [82/160], Loss: 1.0558\n",
      "Phase 1 - Epoch [3/2], Step [83/160], Loss: 0.8909\n",
      "Phase 1 - Epoch [3/2], Step [84/160], Loss: 0.7809\n",
      "Phase 1 - Epoch [3/2], Step [85/160], Loss: 0.9205\n",
      "Phase 1 - Epoch [3/2], Step [86/160], Loss: 0.8102\n",
      "Phase 1 - Epoch [3/2], Step [87/160], Loss: 1.0124\n",
      "Phase 1 - Epoch [3/2], Step [88/160], Loss: 0.8177\n",
      "Phase 1 - Epoch [3/2], Step [89/160], Loss: 0.9110\n",
      "Phase 1 - Epoch [3/2], Step [90/160], Loss: 0.9790\n",
      "Phase 1 - Epoch [3/2], Step [91/160], Loss: 0.7485\n",
      "Phase 1 - Epoch [3/2], Step [92/160], Loss: 0.9462\n",
      "Phase 1 - Epoch [3/2], Step [93/160], Loss: 0.8926\n",
      "Phase 1 - Epoch [3/2], Step [94/160], Loss: 0.7746\n",
      "Phase 1 - Epoch [3/2], Step [95/160], Loss: 1.0787\n",
      "Phase 1 - Epoch [3/2], Step [96/160], Loss: 0.9333\n",
      "Phase 1 - Epoch [3/2], Step [97/160], Loss: 1.2671\n",
      "Phase 1 - Epoch [3/2], Step [98/160], Loss: 0.9487\n",
      "Phase 1 - Epoch [3/2], Step [99/160], Loss: 0.9534\n",
      "Phase 1 - Epoch [3/2], Step [100/160], Loss: 0.9520\n",
      "Phase 1 - Epoch [3/2], Step [101/160], Loss: 0.9775\n",
      "Phase 1 - Epoch [3/2], Step [102/160], Loss: 1.2105\n",
      "Phase 1 - Epoch [3/2], Step [103/160], Loss: 0.8420\n",
      "Phase 1 - Epoch [3/2], Step [104/160], Loss: 0.8373\n",
      "Phase 1 - Epoch [3/2], Step [105/160], Loss: 0.9222\n",
      "Phase 1 - Epoch [3/2], Step [106/160], Loss: 0.8868\n",
      "Phase 1 - Epoch [3/2], Step [107/160], Loss: 0.9797\n",
      "Phase 1 - Epoch [3/2], Step [108/160], Loss: 0.9832\n",
      "Phase 1 - Epoch [3/2], Step [109/160], Loss: 1.0216\n",
      "Phase 1 - Epoch [3/2], Step [110/160], Loss: 0.9384\n",
      "Phase 1 - Epoch [3/2], Step [111/160], Loss: 0.8445\n",
      "Phase 1 - Epoch [3/2], Step [112/160], Loss: 0.9729\n",
      "Phase 1 - Epoch [3/2], Step [113/160], Loss: 0.9424\n",
      "Phase 1 - Epoch [3/2], Step [114/160], Loss: 0.8506\n",
      "Phase 1 - Epoch [3/2], Step [115/160], Loss: 0.8868\n",
      "Phase 1 - Epoch [3/2], Step [116/160], Loss: 0.9566\n",
      "Phase 1 - Epoch [3/2], Step [117/160], Loss: 0.9473\n",
      "Phase 1 - Epoch [3/2], Step [118/160], Loss: 0.8394\n",
      "Phase 1 - Epoch [3/2], Step [119/160], Loss: 0.9430\n",
      "Phase 1 - Epoch [3/2], Step [120/160], Loss: 1.0176\n",
      "Phase 1 - Epoch [3/2], Step [121/160], Loss: 0.9647\n",
      "Phase 1 - Epoch [3/2], Step [122/160], Loss: 0.8678\n",
      "Phase 1 - Epoch [3/2], Step [123/160], Loss: 0.8623\n",
      "Phase 1 - Epoch [3/2], Step [124/160], Loss: 0.8302\n",
      "Phase 1 - Epoch [3/2], Step [125/160], Loss: 1.0021\n",
      "Phase 1 - Epoch [3/2], Step [126/160], Loss: 0.8410\n",
      "Phase 1 - Epoch [3/2], Step [127/160], Loss: 0.8732\n",
      "Phase 1 - Epoch [3/2], Step [128/160], Loss: 1.0439\n",
      "Phase 1 - Epoch [3/2], Step [129/160], Loss: 0.8680\n",
      "Phase 1 - Epoch [3/2], Step [130/160], Loss: 1.1171\n",
      "Phase 1 - Epoch [3/2], Step [131/160], Loss: 0.8668\n",
      "Phase 1 - Epoch [3/2], Step [132/160], Loss: 0.8716\n",
      "Phase 1 - Epoch [3/2], Step [133/160], Loss: 0.9212\n",
      "Phase 1 - Epoch [3/2], Step [134/160], Loss: 0.7616\n",
      "Phase 1 - Epoch [3/2], Step [135/160], Loss: 0.8592\n",
      "Phase 1 - Epoch [3/2], Step [136/160], Loss: 0.8264\n",
      "Phase 1 - Epoch [3/2], Step [137/160], Loss: 0.8339\n",
      "Phase 1 - Epoch [3/2], Step [138/160], Loss: 0.9724\n",
      "Phase 1 - Epoch [3/2], Step [139/160], Loss: 0.7950\n",
      "Phase 1 - Epoch [3/2], Step [140/160], Loss: 0.9405\n",
      "Phase 1 - Epoch [3/2], Step [141/160], Loss: 0.9001\n",
      "Phase 1 - Epoch [3/2], Step [142/160], Loss: 0.9314\n",
      "Phase 1 - Epoch [3/2], Step [143/160], Loss: 1.0192\n",
      "Phase 1 - Epoch [3/2], Step [144/160], Loss: 0.8729\n",
      "Phase 1 - Epoch [3/2], Step [145/160], Loss: 0.8866\n",
      "Phase 1 - Epoch [3/2], Step [146/160], Loss: 0.8665\n",
      "Phase 1 - Epoch [3/2], Step [147/160], Loss: 1.0055\n",
      "Phase 1 - Epoch [3/2], Step [148/160], Loss: 0.8062\n",
      "Phase 1 - Epoch [3/2], Step [149/160], Loss: 0.8596\n",
      "Phase 1 - Epoch [3/2], Step [150/160], Loss: 0.9785\n",
      "Phase 1 - Epoch [3/2], Step [151/160], Loss: 0.9074\n",
      "Phase 1 - Epoch [3/2], Step [152/160], Loss: 0.8720\n",
      "Phase 1 - Epoch [3/2], Step [153/160], Loss: 0.8567\n",
      "Phase 1 - Epoch [3/2], Step [154/160], Loss: 0.8856\n",
      "Phase 1 - Epoch [3/2], Step [155/160], Loss: 0.8985\n",
      "Phase 1 - Epoch [3/2], Step [156/160], Loss: 1.0875\n",
      "Phase 1 - Epoch [3/2], Step [157/160], Loss: 0.9899\n",
      "Phase 1 - Epoch [3/2], Step [158/160], Loss: 0.9910\n",
      "Phase 1 - Epoch [3/2], Step [159/160], Loss: 0.7754\n",
      "Phase 1 - Epoch [3/2], Step [160/160], Loss: 0.8876\n",
      "Validation Loss: 0.9123\n",
      "Train F1: 0.5498, Train Recall: 0.5500, Train Accuracy: 0.5500\n",
      "Val F1: 0.5708, Val Recall: 0.5708, Val Accuracy: 0.5708\n",
      "Best recall so far: 0.5707584050039093, Best parameters so far: {'dropout': 0.3, 'lr': 0.001}\n",
      "Phase 1 - Epoch [4/2], Step [1/160], Loss: 0.8984\n",
      "Phase 1 - Epoch [4/2], Step [2/160], Loss: 0.8271\n",
      "Phase 1 - Epoch [4/2], Step [3/160], Loss: 0.9032\n",
      "Phase 1 - Epoch [4/2], Step [4/160], Loss: 0.8451\n",
      "Phase 1 - Epoch [4/2], Step [5/160], Loss: 0.8904\n",
      "Phase 1 - Epoch [4/2], Step [6/160], Loss: 0.8651\n",
      "Phase 1 - Epoch [4/2], Step [7/160], Loss: 1.0264\n",
      "Phase 1 - Epoch [4/2], Step [8/160], Loss: 0.9245\n",
      "Phase 1 - Epoch [4/2], Step [9/160], Loss: 0.7694\n",
      "Phase 1 - Epoch [4/2], Step [10/160], Loss: 0.9574\n",
      "Phase 1 - Epoch [4/2], Step [11/160], Loss: 0.9799\n",
      "Phase 1 - Epoch [4/2], Step [12/160], Loss: 0.7918\n",
      "Phase 1 - Epoch [4/2], Step [13/160], Loss: 0.8752\n",
      "Phase 1 - Epoch [4/2], Step [14/160], Loss: 0.8688\n",
      "Phase 1 - Epoch [4/2], Step [15/160], Loss: 0.7914\n",
      "Phase 1 - Epoch [4/2], Step [16/160], Loss: 1.1640\n",
      "Phase 1 - Epoch [4/2], Step [17/160], Loss: 1.0635\n",
      "Phase 1 - Epoch [4/2], Step [18/160], Loss: 1.0507\n",
      "Phase 1 - Epoch [4/2], Step [19/160], Loss: 0.8474\n",
      "Phase 1 - Epoch [4/2], Step [20/160], Loss: 0.9667\n",
      "Phase 1 - Epoch [4/2], Step [21/160], Loss: 1.1892\n",
      "Phase 1 - Epoch [4/2], Step [22/160], Loss: 0.8846\n",
      "Phase 1 - Epoch [4/2], Step [23/160], Loss: 0.8326\n",
      "Phase 1 - Epoch [4/2], Step [24/160], Loss: 0.9643\n",
      "Phase 1 - Epoch [4/2], Step [25/160], Loss: 0.8945\n",
      "Phase 1 - Epoch [4/2], Step [26/160], Loss: 0.8373\n",
      "Phase 1 - Epoch [4/2], Step [27/160], Loss: 0.9271\n",
      "Phase 1 - Epoch [4/2], Step [28/160], Loss: 0.7154\n",
      "Phase 1 - Epoch [4/2], Step [29/160], Loss: 0.8268\n",
      "Phase 1 - Epoch [4/2], Step [30/160], Loss: 0.9429\n",
      "Phase 1 - Epoch [4/2], Step [31/160], Loss: 0.9445\n",
      "Phase 1 - Epoch [4/2], Step [32/160], Loss: 0.9444\n",
      "Phase 1 - Epoch [4/2], Step [33/160], Loss: 0.8166\n",
      "Phase 1 - Epoch [4/2], Step [34/160], Loss: 0.9459\n",
      "Phase 1 - Epoch [4/2], Step [35/160], Loss: 0.9665\n",
      "Phase 1 - Epoch [4/2], Step [36/160], Loss: 1.0003\n",
      "Phase 1 - Epoch [4/2], Step [37/160], Loss: 0.9632\n",
      "Phase 1 - Epoch [4/2], Step [38/160], Loss: 0.8850\n",
      "Phase 1 - Epoch [4/2], Step [39/160], Loss: 0.7422\n",
      "Phase 1 - Epoch [4/2], Step [40/160], Loss: 1.0598\n",
      "Phase 1 - Epoch [4/2], Step [41/160], Loss: 1.0175\n",
      "Phase 1 - Epoch [4/2], Step [42/160], Loss: 0.8408\n",
      "Phase 1 - Epoch [4/2], Step [43/160], Loss: 1.0158\n",
      "Phase 1 - Epoch [4/2], Step [44/160], Loss: 0.8603\n",
      "Phase 1 - Epoch [4/2], Step [45/160], Loss: 0.7322\n",
      "Phase 1 - Epoch [4/2], Step [46/160], Loss: 0.7174\n",
      "Phase 1 - Epoch [4/2], Step [47/160], Loss: 0.8030\n",
      "Phase 1 - Epoch [4/2], Step [48/160], Loss: 0.9496\n",
      "Phase 1 - Epoch [4/2], Step [49/160], Loss: 0.9843\n",
      "Phase 1 - Epoch [4/2], Step [50/160], Loss: 0.9003\n",
      "Phase 1 - Epoch [4/2], Step [51/160], Loss: 0.9756\n",
      "Phase 1 - Epoch [4/2], Step [52/160], Loss: 0.7424\n",
      "Phase 1 - Epoch [4/2], Step [53/160], Loss: 0.8975\n",
      "Phase 1 - Epoch [4/2], Step [54/160], Loss: 0.9547\n",
      "Phase 1 - Epoch [4/2], Step [55/160], Loss: 0.7627\n",
      "Phase 1 - Epoch [4/2], Step [56/160], Loss: 0.9500\n",
      "Phase 1 - Epoch [4/2], Step [57/160], Loss: 0.8911\n",
      "Phase 1 - Epoch [4/2], Step [58/160], Loss: 1.1045\n",
      "Phase 1 - Epoch [4/2], Step [59/160], Loss: 1.1441\n",
      "Phase 1 - Epoch [4/2], Step [60/160], Loss: 0.8082\n",
      "Phase 1 - Epoch [4/2], Step [61/160], Loss: 1.1069\n",
      "Phase 1 - Epoch [4/2], Step [62/160], Loss: 0.8083\n",
      "Phase 1 - Epoch [4/2], Step [63/160], Loss: 0.8668\n",
      "Phase 1 - Epoch [4/2], Step [64/160], Loss: 0.9537\n",
      "Phase 1 - Epoch [4/2], Step [65/160], Loss: 0.8507\n",
      "Phase 1 - Epoch [4/2], Step [66/160], Loss: 0.9484\n",
      "Phase 1 - Epoch [4/2], Step [67/160], Loss: 0.7949\n",
      "Phase 1 - Epoch [4/2], Step [68/160], Loss: 0.8659\n",
      "Phase 1 - Epoch [4/2], Step [69/160], Loss: 0.8738\n",
      "Phase 1 - Epoch [4/2], Step [70/160], Loss: 1.0141\n",
      "Phase 1 - Epoch [4/2], Step [71/160], Loss: 0.8529\n",
      "Phase 1 - Epoch [4/2], Step [72/160], Loss: 1.0369\n",
      "Phase 1 - Epoch [4/2], Step [73/160], Loss: 0.6890\n",
      "Phase 1 - Epoch [4/2], Step [74/160], Loss: 0.8617\n",
      "Phase 1 - Epoch [4/2], Step [75/160], Loss: 0.9363\n",
      "Phase 1 - Epoch [4/2], Step [76/160], Loss: 0.8409\n",
      "Phase 1 - Epoch [4/2], Step [77/160], Loss: 0.9072\n",
      "Phase 1 - Epoch [4/2], Step [78/160], Loss: 0.9021\n",
      "Phase 1 - Epoch [4/2], Step [79/160], Loss: 0.8312\n",
      "Phase 1 - Epoch [4/2], Step [80/160], Loss: 0.8744\n",
      "Phase 1 - Epoch [4/2], Step [81/160], Loss: 0.9275\n",
      "Phase 1 - Epoch [4/2], Step [82/160], Loss: 0.8059\n",
      "Phase 1 - Epoch [4/2], Step [83/160], Loss: 0.8547\n",
      "Phase 1 - Epoch [4/2], Step [84/160], Loss: 0.8326\n",
      "Phase 1 - Epoch [4/2], Step [85/160], Loss: 0.8335\n",
      "Phase 1 - Epoch [4/2], Step [86/160], Loss: 0.8014\n",
      "Phase 1 - Epoch [4/2], Step [87/160], Loss: 1.0965\n",
      "Phase 1 - Epoch [4/2], Step [88/160], Loss: 0.9969\n",
      "Phase 1 - Epoch [4/2], Step [89/160], Loss: 0.9203\n",
      "Phase 1 - Epoch [4/2], Step [90/160], Loss: 0.9780\n",
      "Phase 1 - Epoch [4/2], Step [91/160], Loss: 0.9136\n",
      "Phase 1 - Epoch [4/2], Step [92/160], Loss: 0.7994\n",
      "Phase 1 - Epoch [4/2], Step [93/160], Loss: 0.8260\n",
      "Phase 1 - Epoch [4/2], Step [94/160], Loss: 0.8293\n",
      "Phase 1 - Epoch [4/2], Step [95/160], Loss: 1.0937\n",
      "Phase 1 - Epoch [4/2], Step [96/160], Loss: 0.7308\n",
      "Phase 1 - Epoch [4/2], Step [97/160], Loss: 0.7402\n",
      "Phase 1 - Epoch [4/2], Step [98/160], Loss: 0.8318\n",
      "Phase 1 - Epoch [4/2], Step [99/160], Loss: 0.8943\n",
      "Phase 1 - Epoch [4/2], Step [100/160], Loss: 0.7762\n",
      "Phase 1 - Epoch [4/2], Step [101/160], Loss: 0.7682\n",
      "Phase 1 - Epoch [4/2], Step [102/160], Loss: 0.9712\n",
      "Phase 1 - Epoch [4/2], Step [103/160], Loss: 0.8533\n",
      "Phase 1 - Epoch [4/2], Step [104/160], Loss: 0.8266\n",
      "Phase 1 - Epoch [4/2], Step [105/160], Loss: 0.8365\n",
      "Phase 1 - Epoch [4/2], Step [106/160], Loss: 0.8235\n",
      "Phase 1 - Epoch [4/2], Step [107/160], Loss: 0.9749\n",
      "Phase 1 - Epoch [4/2], Step [108/160], Loss: 1.2955\n",
      "Phase 1 - Epoch [4/2], Step [109/160], Loss: 0.7313\n",
      "Phase 1 - Epoch [4/2], Step [110/160], Loss: 0.9558\n",
      "Phase 1 - Epoch [4/2], Step [111/160], Loss: 1.1692\n",
      "Phase 1 - Epoch [4/2], Step [112/160], Loss: 0.7208\n",
      "Phase 1 - Epoch [4/2], Step [113/160], Loss: 0.9512\n",
      "Phase 1 - Epoch [4/2], Step [114/160], Loss: 0.9116\n",
      "Phase 1 - Epoch [4/2], Step [115/160], Loss: 0.7338\n",
      "Phase 1 - Epoch [4/2], Step [116/160], Loss: 0.7880\n",
      "Phase 1 - Epoch [4/2], Step [117/160], Loss: 0.8186\n",
      "Phase 1 - Epoch [4/2], Step [118/160], Loss: 0.8911\n",
      "Phase 1 - Epoch [4/2], Step [119/160], Loss: 0.8485\n",
      "Phase 1 - Epoch [4/2], Step [120/160], Loss: 0.8549\n",
      "Phase 1 - Epoch [4/2], Step [121/160], Loss: 0.6574\n",
      "Phase 1 - Epoch [4/2], Step [122/160], Loss: 0.7756\n",
      "Phase 1 - Epoch [4/2], Step [123/160], Loss: 0.7173\n",
      "Phase 1 - Epoch [4/2], Step [124/160], Loss: 0.8670\n",
      "Phase 1 - Epoch [4/2], Step [125/160], Loss: 0.9731\n",
      "Phase 1 - Epoch [4/2], Step [126/160], Loss: 1.0551\n",
      "Phase 1 - Epoch [4/2], Step [127/160], Loss: 1.0767\n",
      "Phase 1 - Epoch [4/2], Step [128/160], Loss: 0.7296\n",
      "Phase 1 - Epoch [4/2], Step [129/160], Loss: 0.7531\n",
      "Phase 1 - Epoch [4/2], Step [130/160], Loss: 0.9252\n",
      "Phase 1 - Epoch [4/2], Step [131/160], Loss: 0.9619\n",
      "Phase 1 - Epoch [4/2], Step [132/160], Loss: 0.7674\n",
      "Phase 1 - Epoch [4/2], Step [133/160], Loss: 0.8360\n",
      "Phase 1 - Epoch [4/2], Step [134/160], Loss: 1.0365\n",
      "Phase 1 - Epoch [4/2], Step [135/160], Loss: 0.8691\n",
      "Phase 1 - Epoch [4/2], Step [136/160], Loss: 0.9568\n",
      "Phase 1 - Epoch [4/2], Step [137/160], Loss: 0.7868\n",
      "Phase 1 - Epoch [4/2], Step [138/160], Loss: 0.7765\n",
      "Phase 1 - Epoch [4/2], Step [139/160], Loss: 1.0265\n",
      "Phase 1 - Epoch [4/2], Step [140/160], Loss: 0.7454\n",
      "Phase 1 - Epoch [4/2], Step [141/160], Loss: 0.8333\n",
      "Phase 1 - Epoch [4/2], Step [142/160], Loss: 0.9360\n",
      "Phase 1 - Epoch [4/2], Step [143/160], Loss: 0.9520\n",
      "Phase 1 - Epoch [4/2], Step [144/160], Loss: 0.8206\n",
      "Phase 1 - Epoch [4/2], Step [145/160], Loss: 0.9885\n",
      "Phase 1 - Epoch [4/2], Step [146/160], Loss: 0.8719\n",
      "Phase 1 - Epoch [4/2], Step [147/160], Loss: 0.7970\n",
      "Phase 1 - Epoch [4/2], Step [148/160], Loss: 0.8837\n",
      "Phase 1 - Epoch [4/2], Step [149/160], Loss: 0.9549\n",
      "Phase 1 - Epoch [4/2], Step [150/160], Loss: 0.9397\n",
      "Phase 1 - Epoch [4/2], Step [151/160], Loss: 0.9905\n",
      "Phase 1 - Epoch [4/2], Step [152/160], Loss: 0.8355\n",
      "Phase 1 - Epoch [4/2], Step [153/160], Loss: 0.8033\n",
      "Phase 1 - Epoch [4/2], Step [154/160], Loss: 0.7794\n",
      "Phase 1 - Epoch [4/2], Step [155/160], Loss: 0.9901\n",
      "Phase 1 - Epoch [4/2], Step [156/160], Loss: 0.8649\n",
      "Phase 1 - Epoch [4/2], Step [157/160], Loss: 0.8497\n",
      "Phase 1 - Epoch [4/2], Step [158/160], Loss: 1.1352\n",
      "Phase 1 - Epoch [4/2], Step [159/160], Loss: 0.9106\n",
      "Phase 1 - Epoch [4/2], Step [160/160], Loss: 0.8405\n",
      "Validation Loss: 0.9112\n",
      "Train F1: 0.5718, Train Recall: 0.5709, Train Accuracy: 0.5709\n",
      "Val F1: 0.5583, Val Recall: 0.5590, Val Accuracy: 0.5590\n",
      "Best recall so far: 0.5707584050039093, Best parameters so far: {'dropout': 0.3, 'lr': 0.001}\n",
      "Phase 1 - Epoch [5/2], Step [1/160], Loss: 0.8668\n",
      "Phase 1 - Epoch [5/2], Step [2/160], Loss: 0.8548\n",
      "Phase 1 - Epoch [5/2], Step [3/160], Loss: 0.8469\n",
      "Phase 1 - Epoch [5/2], Step [4/160], Loss: 0.8405\n",
      "Phase 1 - Epoch [5/2], Step [5/160], Loss: 0.7680\n",
      "Phase 1 - Epoch [5/2], Step [6/160], Loss: 0.7660\n",
      "Phase 1 - Epoch [5/2], Step [7/160], Loss: 0.5992\n",
      "Phase 1 - Epoch [5/2], Step [8/160], Loss: 0.9212\n",
      "Phase 1 - Epoch [5/2], Step [9/160], Loss: 0.8397\n",
      "Phase 1 - Epoch [5/2], Step [10/160], Loss: 0.8423\n",
      "Phase 1 - Epoch [5/2], Step [11/160], Loss: 0.7248\n",
      "Phase 1 - Epoch [5/2], Step [12/160], Loss: 0.7610\n",
      "Phase 1 - Epoch [5/2], Step [13/160], Loss: 0.6873\n",
      "Phase 1 - Epoch [5/2], Step [14/160], Loss: 0.8320\n",
      "Phase 1 - Epoch [5/2], Step [15/160], Loss: 0.7487\n",
      "Phase 1 - Epoch [5/2], Step [16/160], Loss: 0.5874\n",
      "Phase 1 - Epoch [5/2], Step [17/160], Loss: 0.9795\n",
      "Phase 1 - Epoch [5/2], Step [18/160], Loss: 0.7572\n",
      "Phase 1 - Epoch [5/2], Step [19/160], Loss: 0.8420\n",
      "Phase 1 - Epoch [5/2], Step [20/160], Loss: 0.7477\n",
      "Phase 1 - Epoch [5/2], Step [21/160], Loss: 0.9008\n",
      "Phase 1 - Epoch [5/2], Step [22/160], Loss: 0.8439\n",
      "Phase 1 - Epoch [5/2], Step [23/160], Loss: 0.7093\n",
      "Phase 1 - Epoch [5/2], Step [24/160], Loss: 0.9983\n",
      "Phase 1 - Epoch [5/2], Step [25/160], Loss: 0.7565\n",
      "Phase 1 - Epoch [5/2], Step [26/160], Loss: 0.8254\n",
      "Phase 1 - Epoch [5/2], Step [27/160], Loss: 0.7719\n",
      "Phase 1 - Epoch [5/2], Step [28/160], Loss: 0.9473\n",
      "Phase 1 - Epoch [5/2], Step [29/160], Loss: 0.7820\n",
      "Phase 1 - Epoch [5/2], Step [30/160], Loss: 1.0427\n",
      "Phase 1 - Epoch [5/2], Step [31/160], Loss: 0.7776\n",
      "Phase 1 - Epoch [5/2], Step [32/160], Loss: 0.7908\n",
      "Phase 1 - Epoch [5/2], Step [33/160], Loss: 0.7633\n",
      "Phase 1 - Epoch [5/2], Step [34/160], Loss: 0.8605\n",
      "Phase 1 - Epoch [5/2], Step [35/160], Loss: 0.7749\n",
      "Phase 1 - Epoch [5/2], Step [36/160], Loss: 0.8136\n",
      "Phase 1 - Epoch [5/2], Step [37/160], Loss: 0.9571\n",
      "Phase 1 - Epoch [5/2], Step [38/160], Loss: 0.9247\n",
      "Phase 1 - Epoch [5/2], Step [39/160], Loss: 0.9570\n",
      "Phase 1 - Epoch [5/2], Step [40/160], Loss: 0.8944\n",
      "Phase 1 - Epoch [5/2], Step [41/160], Loss: 0.8109\n",
      "Phase 1 - Epoch [5/2], Step [42/160], Loss: 0.9125\n",
      "Phase 1 - Epoch [5/2], Step [43/160], Loss: 0.8188\n",
      "Phase 1 - Epoch [5/2], Step [44/160], Loss: 0.6501\n",
      "Phase 1 - Epoch [5/2], Step [45/160], Loss: 0.7239\n",
      "Phase 1 - Epoch [5/2], Step [46/160], Loss: 0.9234\n",
      "Phase 1 - Epoch [5/2], Step [47/160], Loss: 0.9959\n",
      "Phase 1 - Epoch [5/2], Step [48/160], Loss: 0.7909\n",
      "Phase 1 - Epoch [5/2], Step [49/160], Loss: 0.6997\n",
      "Phase 1 - Epoch [5/2], Step [50/160], Loss: 0.7629\n",
      "Phase 1 - Epoch [5/2], Step [51/160], Loss: 0.8411\n",
      "Phase 1 - Epoch [5/2], Step [52/160], Loss: 0.8369\n",
      "Phase 1 - Epoch [5/2], Step [53/160], Loss: 0.8380\n",
      "Phase 1 - Epoch [5/2], Step [54/160], Loss: 0.6906\n",
      "Phase 1 - Epoch [5/2], Step [55/160], Loss: 1.0045\n",
      "Phase 1 - Epoch [5/2], Step [56/160], Loss: 0.7120\n",
      "Phase 1 - Epoch [5/2], Step [57/160], Loss: 0.8935\n",
      "Phase 1 - Epoch [5/2], Step [58/160], Loss: 0.7809\n",
      "Phase 1 - Epoch [5/2], Step [59/160], Loss: 0.9925\n",
      "Phase 1 - Epoch [5/2], Step [60/160], Loss: 0.9287\n",
      "Phase 1 - Epoch [5/2], Step [61/160], Loss: 1.0155\n",
      "Phase 1 - Epoch [5/2], Step [62/160], Loss: 0.9029\n",
      "Phase 1 - Epoch [5/2], Step [63/160], Loss: 0.9115\n",
      "Phase 1 - Epoch [5/2], Step [64/160], Loss: 0.7442\n",
      "Phase 1 - Epoch [5/2], Step [65/160], Loss: 0.8574\n",
      "Phase 1 - Epoch [5/2], Step [66/160], Loss: 0.8904\n",
      "Phase 1 - Epoch [5/2], Step [67/160], Loss: 0.8775\n",
      "Phase 1 - Epoch [5/2], Step [68/160], Loss: 0.9168\n",
      "Phase 1 - Epoch [5/2], Step [69/160], Loss: 0.7886\n",
      "Phase 1 - Epoch [5/2], Step [70/160], Loss: 0.7959\n",
      "Phase 1 - Epoch [5/2], Step [71/160], Loss: 0.7540\n",
      "Phase 1 - Epoch [5/2], Step [72/160], Loss: 0.9158\n",
      "Phase 1 - Epoch [5/2], Step [73/160], Loss: 0.8852\n",
      "Phase 1 - Epoch [5/2], Step [74/160], Loss: 0.7543\n",
      "Phase 1 - Epoch [5/2], Step [75/160], Loss: 0.7497\n",
      "Phase 1 - Epoch [5/2], Step [76/160], Loss: 0.9704\n",
      "Phase 1 - Epoch [5/2], Step [77/160], Loss: 0.7643\n",
      "Phase 1 - Epoch [5/2], Step [78/160], Loss: 1.1122\n",
      "Phase 1 - Epoch [5/2], Step [79/160], Loss: 0.9192\n",
      "Phase 1 - Epoch [5/2], Step [80/160], Loss: 0.8076\n",
      "Phase 1 - Epoch [5/2], Step [81/160], Loss: 0.8285\n",
      "Phase 1 - Epoch [5/2], Step [82/160], Loss: 0.9112\n",
      "Phase 1 - Epoch [5/2], Step [83/160], Loss: 0.7289\n",
      "Phase 1 - Epoch [5/2], Step [84/160], Loss: 0.7852\n",
      "Phase 1 - Epoch [5/2], Step [85/160], Loss: 0.9253\n",
      "Phase 1 - Epoch [5/2], Step [86/160], Loss: 0.7346\n",
      "Phase 1 - Epoch [5/2], Step [87/160], Loss: 0.9077\n",
      "Phase 1 - Epoch [5/2], Step [88/160], Loss: 0.6775\n",
      "Phase 1 - Epoch [5/2], Step [89/160], Loss: 0.9360\n",
      "Phase 1 - Epoch [5/2], Step [90/160], Loss: 0.8693\n",
      "Phase 1 - Epoch [5/2], Step [91/160], Loss: 0.8206\n",
      "Phase 1 - Epoch [5/2], Step [92/160], Loss: 0.9135\n",
      "Phase 1 - Epoch [5/2], Step [93/160], Loss: 0.8512\n",
      "Phase 1 - Epoch [5/2], Step [94/160], Loss: 0.6544\n",
      "Phase 1 - Epoch [5/2], Step [95/160], Loss: 1.0068\n",
      "Phase 1 - Epoch [5/2], Step [96/160], Loss: 0.6692\n",
      "Phase 1 - Epoch [5/2], Step [97/160], Loss: 0.7889\n",
      "Phase 1 - Epoch [5/2], Step [98/160], Loss: 0.8533\n",
      "Phase 1 - Epoch [5/2], Step [99/160], Loss: 0.7904\n",
      "Phase 1 - Epoch [5/2], Step [100/160], Loss: 0.9079\n",
      "Phase 1 - Epoch [5/2], Step [101/160], Loss: 0.7386\n",
      "Phase 1 - Epoch [5/2], Step [102/160], Loss: 0.9073\n",
      "Phase 1 - Epoch [5/2], Step [103/160], Loss: 0.6659\n",
      "Phase 1 - Epoch [5/2], Step [104/160], Loss: 0.9199\n",
      "Phase 1 - Epoch [5/2], Step [105/160], Loss: 0.8838\n",
      "Phase 1 - Epoch [5/2], Step [106/160], Loss: 0.7636\n",
      "Phase 1 - Epoch [5/2], Step [107/160], Loss: 0.8061\n",
      "Phase 1 - Epoch [5/2], Step [108/160], Loss: 0.8349\n",
      "Phase 1 - Epoch [5/2], Step [109/160], Loss: 0.8004\n",
      "Phase 1 - Epoch [5/2], Step [110/160], Loss: 0.8639\n",
      "Phase 1 - Epoch [5/2], Step [111/160], Loss: 1.0038\n",
      "Phase 1 - Epoch [5/2], Step [112/160], Loss: 0.9339\n",
      "Phase 1 - Epoch [5/2], Step [113/160], Loss: 1.0605\n",
      "Phase 1 - Epoch [5/2], Step [114/160], Loss: 0.6802\n",
      "Phase 1 - Epoch [5/2], Step [115/160], Loss: 0.7273\n",
      "Phase 1 - Epoch [5/2], Step [116/160], Loss: 1.0238\n",
      "Phase 1 - Epoch [5/2], Step [117/160], Loss: 0.6909\n",
      "Phase 1 - Epoch [5/2], Step [118/160], Loss: 0.8418\n",
      "Phase 1 - Epoch [5/2], Step [119/160], Loss: 0.9211\n",
      "Phase 1 - Epoch [5/2], Step [120/160], Loss: 0.9343\n",
      "Phase 1 - Epoch [5/2], Step [121/160], Loss: 0.9082\n",
      "Phase 1 - Epoch [5/2], Step [122/160], Loss: 0.9675\n",
      "Phase 1 - Epoch [5/2], Step [123/160], Loss: 0.8603\n",
      "Phase 1 - Epoch [5/2], Step [124/160], Loss: 0.8754\n",
      "Phase 1 - Epoch [5/2], Step [125/160], Loss: 0.8595\n",
      "Phase 1 - Epoch [5/2], Step [126/160], Loss: 0.8229\n",
      "Phase 1 - Epoch [5/2], Step [127/160], Loss: 0.8030\n",
      "Phase 1 - Epoch [5/2], Step [128/160], Loss: 0.8483\n",
      "Phase 1 - Epoch [5/2], Step [129/160], Loss: 0.8495\n",
      "Phase 1 - Epoch [5/2], Step [130/160], Loss: 0.8288\n",
      "Phase 1 - Epoch [5/2], Step [131/160], Loss: 0.8610\n",
      "Phase 1 - Epoch [5/2], Step [132/160], Loss: 0.6932\n",
      "Phase 1 - Epoch [5/2], Step [133/160], Loss: 0.9808\n",
      "Phase 1 - Epoch [5/2], Step [134/160], Loss: 0.7932\n",
      "Phase 1 - Epoch [5/2], Step [135/160], Loss: 0.7618\n",
      "Phase 1 - Epoch [5/2], Step [136/160], Loss: 0.7990\n",
      "Phase 1 - Epoch [5/2], Step [137/160], Loss: 0.8810\n",
      "Phase 1 - Epoch [5/2], Step [138/160], Loss: 0.8899\n",
      "Phase 1 - Epoch [5/2], Step [139/160], Loss: 0.8015\n",
      "Phase 1 - Epoch [5/2], Step [140/160], Loss: 0.9387\n",
      "Phase 1 - Epoch [5/2], Step [141/160], Loss: 0.7482\n",
      "Phase 1 - Epoch [5/2], Step [142/160], Loss: 0.8822\n",
      "Phase 1 - Epoch [5/2], Step [143/160], Loss: 0.7378\n",
      "Phase 1 - Epoch [5/2], Step [144/160], Loss: 0.9335\n",
      "Phase 1 - Epoch [5/2], Step [145/160], Loss: 0.7497\n",
      "Phase 1 - Epoch [5/2], Step [146/160], Loss: 0.8620\n",
      "Phase 1 - Epoch [5/2], Step [147/160], Loss: 0.8784\n",
      "Phase 1 - Epoch [5/2], Step [148/160], Loss: 0.9369\n",
      "Phase 1 - Epoch [5/2], Step [149/160], Loss: 0.7818\n",
      "Phase 1 - Epoch [5/2], Step [150/160], Loss: 0.9875\n",
      "Phase 1 - Epoch [5/2], Step [151/160], Loss: 0.8142\n",
      "Phase 1 - Epoch [5/2], Step [152/160], Loss: 0.7862\n",
      "Phase 1 - Epoch [5/2], Step [153/160], Loss: 1.0125\n",
      "Phase 1 - Epoch [5/2], Step [154/160], Loss: 0.8078\n",
      "Phase 1 - Epoch [5/2], Step [155/160], Loss: 0.8830\n",
      "Phase 1 - Epoch [5/2], Step [156/160], Loss: 0.7887\n",
      "Phase 1 - Epoch [5/2], Step [157/160], Loss: 0.6530\n",
      "Phase 1 - Epoch [5/2], Step [158/160], Loss: 0.6740\n",
      "Phase 1 - Epoch [5/2], Step [159/160], Loss: 0.7983\n",
      "Phase 1 - Epoch [5/2], Step [160/160], Loss: 0.9443\n",
      "Validation Loss: 0.9256\n",
      "Train F1: 0.6069, Train Recall: 0.6061, Train Accuracy: 0.6061\n",
      "Val F1: 0.5418, Val Recall: 0.5473, Val Accuracy: 0.5473\n",
      "Best recall so far: 0.5707584050039093, Best parameters so far: {'dropout': 0.3, 'lr': 0.001}\n",
      "Phase 1 - Epoch [6/2], Step [1/160], Loss: 0.8228\n",
      "Phase 1 - Epoch [6/2], Step [2/160], Loss: 0.8087\n",
      "Phase 1 - Epoch [6/2], Step [3/160], Loss: 0.7765\n",
      "Phase 1 - Epoch [6/2], Step [4/160], Loss: 0.6817\n",
      "Phase 1 - Epoch [6/2], Step [5/160], Loss: 0.8071\n",
      "Phase 1 - Epoch [6/2], Step [6/160], Loss: 0.9077\n",
      "Phase 1 - Epoch [6/2], Step [7/160], Loss: 0.5900\n",
      "Phase 1 - Epoch [6/2], Step [8/160], Loss: 0.6263\n",
      "Phase 1 - Epoch [6/2], Step [9/160], Loss: 1.0611\n",
      "Phase 1 - Epoch [6/2], Step [10/160], Loss: 0.7513\n",
      "Phase 1 - Epoch [6/2], Step [11/160], Loss: 0.9192\n",
      "Phase 1 - Epoch [6/2], Step [12/160], Loss: 0.9104\n",
      "Phase 1 - Epoch [6/2], Step [13/160], Loss: 0.8684\n",
      "Phase 1 - Epoch [6/2], Step [14/160], Loss: 0.7934\n",
      "Phase 1 - Epoch [6/2], Step [15/160], Loss: 0.7354\n",
      "Phase 1 - Epoch [6/2], Step [16/160], Loss: 0.7619\n",
      "Phase 1 - Epoch [6/2], Step [17/160], Loss: 0.7270\n",
      "Phase 1 - Epoch [6/2], Step [18/160], Loss: 1.0213\n",
      "Phase 1 - Epoch [6/2], Step [19/160], Loss: 0.9321\n",
      "Phase 1 - Epoch [6/2], Step [20/160], Loss: 0.7425\n",
      "Phase 1 - Epoch [6/2], Step [21/160], Loss: 0.7770\n",
      "Phase 1 - Epoch [6/2], Step [22/160], Loss: 0.8330\n",
      "Phase 1 - Epoch [6/2], Step [23/160], Loss: 0.6920\n",
      "Phase 1 - Epoch [6/2], Step [24/160], Loss: 1.0794\n",
      "Phase 1 - Epoch [6/2], Step [25/160], Loss: 0.8819\n",
      "Phase 1 - Epoch [6/2], Step [26/160], Loss: 0.8414\n",
      "Phase 1 - Epoch [6/2], Step [27/160], Loss: 0.7586\n",
      "Phase 1 - Epoch [6/2], Step [28/160], Loss: 0.8872\n",
      "Phase 1 - Epoch [6/2], Step [29/160], Loss: 0.8105\n",
      "Phase 1 - Epoch [6/2], Step [30/160], Loss: 0.9069\n",
      "Phase 1 - Epoch [6/2], Step [31/160], Loss: 0.8187\n",
      "Phase 1 - Epoch [6/2], Step [32/160], Loss: 0.9249\n",
      "Phase 1 - Epoch [6/2], Step [33/160], Loss: 0.6243\n",
      "Phase 1 - Epoch [6/2], Step [34/160], Loss: 0.9655\n",
      "Phase 1 - Epoch [6/2], Step [35/160], Loss: 0.7854\n",
      "Phase 1 - Epoch [6/2], Step [36/160], Loss: 0.7013\n",
      "Phase 1 - Epoch [6/2], Step [37/160], Loss: 0.7607\n",
      "Phase 1 - Epoch [6/2], Step [38/160], Loss: 0.8141\n",
      "Phase 1 - Epoch [6/2], Step [39/160], Loss: 0.6915\n",
      "Phase 1 - Epoch [6/2], Step [40/160], Loss: 0.8037\n",
      "Phase 1 - Epoch [6/2], Step [41/160], Loss: 0.6237\n",
      "Phase 1 - Epoch [6/2], Step [42/160], Loss: 0.5542\n",
      "Phase 1 - Epoch [6/2], Step [43/160], Loss: 0.7600\n",
      "Phase 1 - Epoch [6/2], Step [44/160], Loss: 0.9107\n",
      "Phase 1 - Epoch [6/2], Step [45/160], Loss: 0.8559\n",
      "Phase 1 - Epoch [6/2], Step [46/160], Loss: 0.8537\n",
      "Phase 1 - Epoch [6/2], Step [47/160], Loss: 0.8085\n",
      "Phase 1 - Epoch [6/2], Step [48/160], Loss: 0.6967\n",
      "Phase 1 - Epoch [6/2], Step [49/160], Loss: 0.6951\n",
      "Phase 1 - Epoch [6/2], Step [50/160], Loss: 0.6307\n",
      "Phase 1 - Epoch [6/2], Step [51/160], Loss: 0.9983\n",
      "Phase 1 - Epoch [6/2], Step [52/160], Loss: 0.7821\n",
      "Phase 1 - Epoch [6/2], Step [53/160], Loss: 0.6311\n",
      "Phase 1 - Epoch [6/2], Step [54/160], Loss: 0.7578\n",
      "Phase 1 - Epoch [6/2], Step [55/160], Loss: 0.7501\n",
      "Phase 1 - Epoch [6/2], Step [56/160], Loss: 0.8384\n",
      "Phase 1 - Epoch [6/2], Step [57/160], Loss: 0.5976\n",
      "Phase 1 - Epoch [6/2], Step [58/160], Loss: 0.5374\n",
      "Phase 1 - Epoch [6/2], Step [59/160], Loss: 0.7671\n",
      "Phase 1 - Epoch [6/2], Step [60/160], Loss: 0.7502\n",
      "Phase 1 - Epoch [6/2], Step [61/160], Loss: 0.9450\n",
      "Phase 1 - Epoch [6/2], Step [62/160], Loss: 0.8583\n",
      "Phase 1 - Epoch [6/2], Step [63/160], Loss: 0.8199\n",
      "Phase 1 - Epoch [6/2], Step [64/160], Loss: 0.7754\n",
      "Phase 1 - Epoch [6/2], Step [65/160], Loss: 0.5929\n",
      "Phase 1 - Epoch [6/2], Step [66/160], Loss: 0.9715\n",
      "Phase 1 - Epoch [6/2], Step [67/160], Loss: 0.8099\n",
      "Phase 1 - Epoch [6/2], Step [68/160], Loss: 0.6669\n",
      "Phase 1 - Epoch [6/2], Step [69/160], Loss: 0.7513\n",
      "Phase 1 - Epoch [6/2], Step [70/160], Loss: 0.7614\n",
      "Phase 1 - Epoch [6/2], Step [71/160], Loss: 0.9974\n",
      "Phase 1 - Epoch [6/2], Step [72/160], Loss: 0.7748\n",
      "Phase 1 - Epoch [6/2], Step [73/160], Loss: 0.7465\n",
      "Phase 1 - Epoch [6/2], Step [74/160], Loss: 0.8366\n",
      "Phase 1 - Epoch [6/2], Step [75/160], Loss: 0.9164\n",
      "Phase 1 - Epoch [6/2], Step [76/160], Loss: 0.7179\n",
      "Phase 1 - Epoch [6/2], Step [77/160], Loss: 0.8179\n",
      "Phase 1 - Epoch [6/2], Step [78/160], Loss: 0.7700\n",
      "Phase 1 - Epoch [6/2], Step [79/160], Loss: 0.6469\n",
      "Phase 1 - Epoch [6/2], Step [80/160], Loss: 0.6973\n",
      "Phase 1 - Epoch [6/2], Step [81/160], Loss: 0.8059\n",
      "Phase 1 - Epoch [6/2], Step [82/160], Loss: 0.6334\n",
      "Phase 1 - Epoch [6/2], Step [83/160], Loss: 0.9617\n",
      "Phase 1 - Epoch [6/2], Step [84/160], Loss: 0.7313\n",
      "Phase 1 - Epoch [6/2], Step [85/160], Loss: 1.0331\n",
      "Phase 1 - Epoch [6/2], Step [86/160], Loss: 0.8924\n",
      "Phase 1 - Epoch [6/2], Step [87/160], Loss: 0.7410\n",
      "Phase 1 - Epoch [6/2], Step [88/160], Loss: 0.9100\n",
      "Phase 1 - Epoch [6/2], Step [89/160], Loss: 0.6539\n",
      "Phase 1 - Epoch [6/2], Step [90/160], Loss: 0.8060\n",
      "Phase 1 - Epoch [6/2], Step [91/160], Loss: 0.9698\n",
      "Phase 1 - Epoch [6/2], Step [92/160], Loss: 0.6862\n",
      "Phase 1 - Epoch [6/2], Step [93/160], Loss: 0.7078\n",
      "Phase 1 - Epoch [6/2], Step [94/160], Loss: 0.9523\n",
      "Phase 1 - Epoch [6/2], Step [95/160], Loss: 0.7399\n",
      "Phase 1 - Epoch [6/2], Step [96/160], Loss: 0.8247\n",
      "Phase 1 - Epoch [6/2], Step [97/160], Loss: 0.8948\n",
      "Phase 1 - Epoch [6/2], Step [98/160], Loss: 0.8203\n",
      "Phase 1 - Epoch [6/2], Step [99/160], Loss: 0.6333\n",
      "Phase 1 - Epoch [6/2], Step [100/160], Loss: 0.7782\n",
      "Phase 1 - Epoch [6/2], Step [101/160], Loss: 0.6498\n",
      "Phase 1 - Epoch [6/2], Step [102/160], Loss: 0.6910\n",
      "Phase 1 - Epoch [6/2], Step [103/160], Loss: 0.7966\n",
      "Phase 1 - Epoch [6/2], Step [104/160], Loss: 0.6195\n",
      "Phase 1 - Epoch [6/2], Step [105/160], Loss: 0.8048\n",
      "Phase 1 - Epoch [6/2], Step [106/160], Loss: 0.6936\n",
      "Phase 1 - Epoch [6/2], Step [107/160], Loss: 0.9152\n",
      "Phase 1 - Epoch [6/2], Step [108/160], Loss: 0.7747\n",
      "Phase 1 - Epoch [6/2], Step [109/160], Loss: 0.8429\n",
      "Phase 1 - Epoch [6/2], Step [110/160], Loss: 0.8926\n",
      "Phase 1 - Epoch [6/2], Step [111/160], Loss: 1.0152\n",
      "Phase 1 - Epoch [6/2], Step [112/160], Loss: 0.8476\n",
      "Phase 1 - Epoch [6/2], Step [113/160], Loss: 0.7413\n",
      "Phase 1 - Epoch [6/2], Step [114/160], Loss: 0.7954\n",
      "Phase 1 - Epoch [6/2], Step [115/160], Loss: 0.7942\n",
      "Phase 1 - Epoch [6/2], Step [116/160], Loss: 0.7977\n",
      "Phase 1 - Epoch [6/2], Step [117/160], Loss: 0.8038\n",
      "Phase 1 - Epoch [6/2], Step [118/160], Loss: 0.9640\n",
      "Phase 1 - Epoch [6/2], Step [119/160], Loss: 0.9024\n",
      "Phase 1 - Epoch [6/2], Step [120/160], Loss: 0.8161\n",
      "Phase 1 - Epoch [6/2], Step [121/160], Loss: 0.8410\n",
      "Phase 1 - Epoch [6/2], Step [122/160], Loss: 0.8496\n",
      "Phase 1 - Epoch [6/2], Step [123/160], Loss: 0.8568\n",
      "Phase 1 - Epoch [6/2], Step [124/160], Loss: 0.9884\n",
      "Phase 1 - Epoch [6/2], Step [125/160], Loss: 0.5763\n",
      "Phase 1 - Epoch [6/2], Step [126/160], Loss: 0.8344\n",
      "Phase 1 - Epoch [6/2], Step [127/160], Loss: 0.8795\n",
      "Phase 1 - Epoch [6/2], Step [128/160], Loss: 0.7605\n",
      "Phase 1 - Epoch [6/2], Step [129/160], Loss: 0.7813\n",
      "Phase 1 - Epoch [6/2], Step [130/160], Loss: 0.8472\n",
      "Phase 1 - Epoch [6/2], Step [131/160], Loss: 0.8149\n",
      "Phase 1 - Epoch [6/2], Step [132/160], Loss: 0.8039\n",
      "Phase 1 - Epoch [6/2], Step [133/160], Loss: 0.8504\n",
      "Phase 1 - Epoch [6/2], Step [134/160], Loss: 0.8475\n",
      "Phase 1 - Epoch [6/2], Step [135/160], Loss: 0.7332\n",
      "Phase 1 - Epoch [6/2], Step [136/160], Loss: 0.7720\n",
      "Phase 1 - Epoch [6/2], Step [137/160], Loss: 0.9048\n",
      "Phase 1 - Epoch [6/2], Step [138/160], Loss: 0.7770\n",
      "Phase 1 - Epoch [6/2], Step [139/160], Loss: 0.8646\n",
      "Phase 1 - Epoch [6/2], Step [140/160], Loss: 0.8842\n",
      "Phase 1 - Epoch [6/2], Step [141/160], Loss: 0.7907\n",
      "Phase 1 - Epoch [6/2], Step [142/160], Loss: 0.6703\n",
      "Phase 1 - Epoch [6/2], Step [143/160], Loss: 0.6932\n",
      "Phase 1 - Epoch [6/2], Step [144/160], Loss: 0.7773\n",
      "Phase 1 - Epoch [6/2], Step [145/160], Loss: 0.9044\n",
      "Phase 1 - Epoch [6/2], Step [146/160], Loss: 0.8610\n",
      "Phase 1 - Epoch [6/2], Step [147/160], Loss: 0.8821\n",
      "Phase 1 - Epoch [6/2], Step [148/160], Loss: 0.7410\n",
      "Phase 1 - Epoch [6/2], Step [149/160], Loss: 0.9185\n",
      "Phase 1 - Epoch [6/2], Step [150/160], Loss: 0.9766\n",
      "Phase 1 - Epoch [6/2], Step [151/160], Loss: 0.7759\n",
      "Phase 1 - Epoch [6/2], Step [152/160], Loss: 0.8487\n",
      "Phase 1 - Epoch [6/2], Step [153/160], Loss: 0.8847\n",
      "Phase 1 - Epoch [6/2], Step [154/160], Loss: 0.9060\n",
      "Phase 1 - Epoch [6/2], Step [155/160], Loss: 0.8652\n",
      "Phase 1 - Epoch [6/2], Step [156/160], Loss: 0.8228\n",
      "Phase 1 - Epoch [6/2], Step [157/160], Loss: 0.8608\n",
      "Phase 1 - Epoch [6/2], Step [158/160], Loss: 0.7985\n",
      "Phase 1 - Epoch [6/2], Step [159/160], Loss: 0.8180\n",
      "Phase 1 - Epoch [6/2], Step [160/160], Loss: 0.7426\n",
      "Validation Loss: 0.9071\n",
      "Train F1: 0.6289, Train Recall: 0.6284, Train Accuracy: 0.6284\n",
      "Val F1: 0.5673, Val Recall: 0.5708, Val Accuracy: 0.5708\n",
      "Best recall so far: 0.5707584050039093, Best parameters so far: {'dropout': 0.3, 'lr': 0.001}\n",
      "Phase 1 - Epoch [7/2], Step [1/160], Loss: 0.6809\n",
      "Phase 1 - Epoch [7/2], Step [2/160], Loss: 0.8079\n",
      "Phase 1 - Epoch [7/2], Step [3/160], Loss: 0.8331\n",
      "Phase 1 - Epoch [7/2], Step [4/160], Loss: 0.6018\n",
      "Phase 1 - Epoch [7/2], Step [5/160], Loss: 0.5694\n",
      "Phase 1 - Epoch [7/2], Step [6/160], Loss: 0.7822\n",
      "Phase 1 - Epoch [7/2], Step [7/160], Loss: 0.7516\n",
      "Phase 1 - Epoch [7/2], Step [8/160], Loss: 0.8371\n",
      "Phase 1 - Epoch [7/2], Step [9/160], Loss: 0.6974\n",
      "Phase 1 - Epoch [7/2], Step [10/160], Loss: 0.7756\n",
      "Phase 1 - Epoch [7/2], Step [11/160], Loss: 0.7115\n",
      "Phase 1 - Epoch [7/2], Step [12/160], Loss: 0.8391\n",
      "Phase 1 - Epoch [7/2], Step [13/160], Loss: 0.5837\n",
      "Phase 1 - Epoch [7/2], Step [14/160], Loss: 0.9999\n",
      "Phase 1 - Epoch [7/2], Step [15/160], Loss: 0.8184\n",
      "Phase 1 - Epoch [7/2], Step [16/160], Loss: 0.8329\n",
      "Phase 1 - Epoch [7/2], Step [17/160], Loss: 0.6324\n",
      "Phase 1 - Epoch [7/2], Step [18/160], Loss: 0.8255\n",
      "Phase 1 - Epoch [7/2], Step [19/160], Loss: 0.5679\n",
      "Phase 1 - Epoch [7/2], Step [20/160], Loss: 0.7508\n",
      "Phase 1 - Epoch [7/2], Step [21/160], Loss: 0.8094\n",
      "Phase 1 - Epoch [7/2], Step [22/160], Loss: 0.6757\n",
      "Phase 1 - Epoch [7/2], Step [23/160], Loss: 1.0342\n",
      "Phase 1 - Epoch [7/2], Step [24/160], Loss: 0.7525\n",
      "Phase 1 - Epoch [7/2], Step [25/160], Loss: 0.6668\n",
      "Phase 1 - Epoch [7/2], Step [26/160], Loss: 0.9658\n",
      "Phase 1 - Epoch [7/2], Step [27/160], Loss: 0.6215\n",
      "Phase 1 - Epoch [7/2], Step [28/160], Loss: 0.9556\n",
      "Phase 1 - Epoch [7/2], Step [29/160], Loss: 0.7199\n",
      "Phase 1 - Epoch [7/2], Step [30/160], Loss: 0.9570\n",
      "Phase 1 - Epoch [7/2], Step [31/160], Loss: 0.4660\n",
      "Phase 1 - Epoch [7/2], Step [32/160], Loss: 0.6893\n",
      "Phase 1 - Epoch [7/2], Step [33/160], Loss: 0.5985\n",
      "Phase 1 - Epoch [7/2], Step [34/160], Loss: 0.6090\n",
      "Phase 1 - Epoch [7/2], Step [35/160], Loss: 0.6737\n",
      "Phase 1 - Epoch [7/2], Step [36/160], Loss: 0.6879\n",
      "Phase 1 - Epoch [7/2], Step [37/160], Loss: 0.7347\n",
      "Phase 1 - Epoch [7/2], Step [38/160], Loss: 0.8872\n",
      "Phase 1 - Epoch [7/2], Step [39/160], Loss: 0.8173\n",
      "Phase 1 - Epoch [7/2], Step [40/160], Loss: 0.8179\n",
      "Phase 1 - Epoch [7/2], Step [41/160], Loss: 0.6187\n",
      "Phase 1 - Epoch [7/2], Step [42/160], Loss: 0.9519\n",
      "Phase 1 - Epoch [7/2], Step [43/160], Loss: 0.7620\n",
      "Phase 1 - Epoch [7/2], Step [44/160], Loss: 0.6841\n",
      "Phase 1 - Epoch [7/2], Step [45/160], Loss: 0.6910\n",
      "Phase 1 - Epoch [7/2], Step [46/160], Loss: 0.6922\n",
      "Phase 1 - Epoch [7/2], Step [47/160], Loss: 0.7764\n",
      "Phase 1 - Epoch [7/2], Step [48/160], Loss: 0.6864\n",
      "Phase 1 - Epoch [7/2], Step [49/160], Loss: 0.7146\n",
      "Phase 1 - Epoch [7/2], Step [50/160], Loss: 0.6071\n",
      "Phase 1 - Epoch [7/2], Step [51/160], Loss: 0.6169\n",
      "Phase 1 - Epoch [7/2], Step [52/160], Loss: 0.7915\n",
      "Phase 1 - Epoch [7/2], Step [53/160], Loss: 0.7184\n",
      "Phase 1 - Epoch [7/2], Step [54/160], Loss: 0.6942\n",
      "Phase 1 - Epoch [7/2], Step [55/160], Loss: 0.8152\n",
      "Phase 1 - Epoch [7/2], Step [56/160], Loss: 0.8284\n",
      "Phase 1 - Epoch [7/2], Step [57/160], Loss: 0.8381\n",
      "Phase 1 - Epoch [7/2], Step [58/160], Loss: 0.7612\n",
      "Phase 1 - Epoch [7/2], Step [59/160], Loss: 0.9198\n",
      "Phase 1 - Epoch [7/2], Step [60/160], Loss: 0.5931\n",
      "Phase 1 - Epoch [7/2], Step [61/160], Loss: 1.0486\n",
      "Phase 1 - Epoch [7/2], Step [62/160], Loss: 0.6809\n",
      "Phase 1 - Epoch [7/2], Step [63/160], Loss: 0.6835\n",
      "Phase 1 - Epoch [7/2], Step [64/160], Loss: 0.8834\n",
      "Phase 1 - Epoch [7/2], Step [65/160], Loss: 0.8545\n",
      "Phase 1 - Epoch [7/2], Step [66/160], Loss: 0.7198\n",
      "Phase 1 - Epoch [7/2], Step [67/160], Loss: 0.8382\n",
      "Phase 1 - Epoch [7/2], Step [68/160], Loss: 0.7880\n",
      "Phase 1 - Epoch [7/2], Step [69/160], Loss: 0.9746\n",
      "Phase 1 - Epoch [7/2], Step [70/160], Loss: 0.6534\n",
      "Phase 1 - Epoch [7/2], Step [71/160], Loss: 0.9464\n",
      "Phase 1 - Epoch [7/2], Step [72/160], Loss: 0.7690\n",
      "Phase 1 - Epoch [7/2], Step [73/160], Loss: 0.8129\n",
      "Phase 1 - Epoch [7/2], Step [74/160], Loss: 0.5912\n",
      "Phase 1 - Epoch [7/2], Step [75/160], Loss: 0.7373\n",
      "Phase 1 - Epoch [7/2], Step [76/160], Loss: 0.7153\n",
      "Phase 1 - Epoch [7/2], Step [77/160], Loss: 0.7575\n",
      "Phase 1 - Epoch [7/2], Step [78/160], Loss: 0.8363\n",
      "Phase 1 - Epoch [7/2], Step [79/160], Loss: 0.8196\n",
      "Phase 1 - Epoch [7/2], Step [80/160], Loss: 0.6405\n",
      "Phase 1 - Epoch [7/2], Step [81/160], Loss: 0.8336\n",
      "Phase 1 - Epoch [7/2], Step [82/160], Loss: 0.8266\n",
      "Phase 1 - Epoch [7/2], Step [83/160], Loss: 0.7684\n",
      "Phase 1 - Epoch [7/2], Step [84/160], Loss: 0.6618\n",
      "Phase 1 - Epoch [7/2], Step [85/160], Loss: 1.0748\n",
      "Phase 1 - Epoch [7/2], Step [86/160], Loss: 0.7774\n",
      "Phase 1 - Epoch [7/2], Step [87/160], Loss: 0.6661\n",
      "Phase 1 - Epoch [7/2], Step [88/160], Loss: 0.6063\n",
      "Phase 1 - Epoch [7/2], Step [89/160], Loss: 0.9306\n",
      "Phase 1 - Epoch [7/2], Step [90/160], Loss: 0.8588\n",
      "Phase 1 - Epoch [7/2], Step [91/160], Loss: 0.7069\n",
      "Phase 1 - Epoch [7/2], Step [92/160], Loss: 0.9316\n",
      "Phase 1 - Epoch [7/2], Step [93/160], Loss: 0.7738\n",
      "Phase 1 - Epoch [7/2], Step [94/160], Loss: 0.7746\n",
      "Phase 1 - Epoch [7/2], Step [95/160], Loss: 0.8220\n",
      "Phase 1 - Epoch [7/2], Step [96/160], Loss: 0.9598\n",
      "Phase 1 - Epoch [7/2], Step [97/160], Loss: 0.6539\n",
      "Phase 1 - Epoch [7/2], Step [98/160], Loss: 0.7766\n",
      "Phase 1 - Epoch [7/2], Step [99/160], Loss: 0.9173\n",
      "Phase 1 - Epoch [7/2], Step [100/160], Loss: 0.7202\n",
      "Phase 1 - Epoch [7/2], Step [101/160], Loss: 0.7154\n",
      "Phase 1 - Epoch [7/2], Step [102/160], Loss: 0.6980\n",
      "Phase 1 - Epoch [7/2], Step [103/160], Loss: 0.8525\n",
      "Phase 1 - Epoch [7/2], Step [104/160], Loss: 0.7659\n",
      "Phase 1 - Epoch [7/2], Step [105/160], Loss: 0.7391\n",
      "Phase 1 - Epoch [7/2], Step [106/160], Loss: 0.5708\n",
      "Phase 1 - Epoch [7/2], Step [107/160], Loss: 0.7880\n",
      "Phase 1 - Epoch [7/2], Step [108/160], Loss: 0.7467\n",
      "Phase 1 - Epoch [7/2], Step [109/160], Loss: 0.6560\n",
      "Phase 1 - Epoch [7/2], Step [110/160], Loss: 0.7887\n",
      "Phase 1 - Epoch [7/2], Step [111/160], Loss: 0.8623\n",
      "Phase 1 - Epoch [7/2], Step [112/160], Loss: 0.6468\n",
      "Phase 1 - Epoch [7/2], Step [113/160], Loss: 0.8175\n",
      "Phase 1 - Epoch [7/2], Step [114/160], Loss: 0.8736\n",
      "Phase 1 - Epoch [7/2], Step [115/160], Loss: 1.0357\n",
      "Phase 1 - Epoch [7/2], Step [116/160], Loss: 0.8875\n",
      "Phase 1 - Epoch [7/2], Step [117/160], Loss: 0.6624\n",
      "Phase 1 - Epoch [7/2], Step [118/160], Loss: 0.4729\n",
      "Phase 1 - Epoch [7/2], Step [119/160], Loss: 0.5885\n",
      "Phase 1 - Epoch [7/2], Step [120/160], Loss: 0.6605\n",
      "Phase 1 - Epoch [7/2], Step [121/160], Loss: 0.5953\n",
      "Phase 1 - Epoch [7/2], Step [122/160], Loss: 0.6821\n",
      "Phase 1 - Epoch [7/2], Step [123/160], Loss: 0.8740\n",
      "Phase 1 - Epoch [7/2], Step [124/160], Loss: 0.8355\n",
      "Phase 1 - Epoch [7/2], Step [125/160], Loss: 0.8390\n",
      "Phase 1 - Epoch [7/2], Step [126/160], Loss: 0.7457\n",
      "Phase 1 - Epoch [7/2], Step [127/160], Loss: 0.6098\n",
      "Phase 1 - Epoch [7/2], Step [128/160], Loss: 1.0438\n",
      "Phase 1 - Epoch [7/2], Step [129/160], Loss: 0.7017\n",
      "Phase 1 - Epoch [7/2], Step [130/160], Loss: 0.6514\n",
      "Phase 1 - Epoch [7/2], Step [131/160], Loss: 0.7397\n",
      "Phase 1 - Epoch [7/2], Step [132/160], Loss: 0.7917\n",
      "Phase 1 - Epoch [7/2], Step [133/160], Loss: 0.7067\n",
      "Phase 1 - Epoch [7/2], Step [134/160], Loss: 0.7100\n",
      "Phase 1 - Epoch [7/2], Step [135/160], Loss: 0.6025\n",
      "Phase 1 - Epoch [7/2], Step [136/160], Loss: 0.9647\n",
      "Phase 1 - Epoch [7/2], Step [137/160], Loss: 0.7577\n",
      "Phase 1 - Epoch [7/2], Step [138/160], Loss: 0.6683\n",
      "Phase 1 - Epoch [7/2], Step [139/160], Loss: 0.9379\n",
      "Phase 1 - Epoch [7/2], Step [140/160], Loss: 1.0616\n",
      "Phase 1 - Epoch [7/2], Step [141/160], Loss: 0.5980\n",
      "Phase 1 - Epoch [7/2], Step [142/160], Loss: 0.6061\n",
      "Phase 1 - Epoch [7/2], Step [143/160], Loss: 0.6927\n",
      "Phase 1 - Epoch [7/2], Step [144/160], Loss: 0.8581\n",
      "Phase 1 - Epoch [7/2], Step [145/160], Loss: 0.6492\n",
      "Phase 1 - Epoch [7/2], Step [146/160], Loss: 0.8042\n",
      "Phase 1 - Epoch [7/2], Step [147/160], Loss: 0.8051\n",
      "Phase 1 - Epoch [7/2], Step [148/160], Loss: 0.6639\n",
      "Phase 1 - Epoch [7/2], Step [149/160], Loss: 0.7612\n",
      "Phase 1 - Epoch [7/2], Step [150/160], Loss: 0.6475\n",
      "Phase 1 - Epoch [7/2], Step [151/160], Loss: 0.6228\n",
      "Phase 1 - Epoch [7/2], Step [152/160], Loss: 0.8203\n",
      "Phase 1 - Epoch [7/2], Step [153/160], Loss: 0.5390\n",
      "Phase 1 - Epoch [7/2], Step [154/160], Loss: 0.8421\n",
      "Phase 1 - Epoch [7/2], Step [155/160], Loss: 0.7967\n",
      "Phase 1 - Epoch [7/2], Step [156/160], Loss: 0.8350\n",
      "Phase 1 - Epoch [7/2], Step [157/160], Loss: 0.4817\n",
      "Phase 1 - Epoch [7/2], Step [158/160], Loss: 0.8733\n",
      "Phase 1 - Epoch [7/2], Step [159/160], Loss: 0.8196\n",
      "Phase 1 - Epoch [7/2], Step [160/160], Loss: 0.7505\n",
      "Validation Loss: 0.9562\n",
      "Train F1: 0.6547, Train Recall: 0.6538, Train Accuracy: 0.6538\n",
      "Val F1: 0.5618, Val Recall: 0.5661, Val Accuracy: 0.5661\n",
      "Best recall so far: 0.5707584050039093, Best parameters so far: {'dropout': 0.3, 'lr': 0.001}\n",
      "Phase 1 - Epoch [8/2], Step [1/160], Loss: 0.7044\n",
      "Phase 1 - Epoch [8/2], Step [2/160], Loss: 0.7264\n",
      "Phase 1 - Epoch [8/2], Step [3/160], Loss: 0.7412\n",
      "Phase 1 - Epoch [8/2], Step [4/160], Loss: 0.5522\n",
      "Phase 1 - Epoch [8/2], Step [5/160], Loss: 0.5700\n",
      "Phase 1 - Epoch [8/2], Step [6/160], Loss: 0.5788\n",
      "Phase 1 - Epoch [8/2], Step [7/160], Loss: 0.7776\n",
      "Phase 1 - Epoch [8/2], Step [8/160], Loss: 1.0435\n",
      "Phase 1 - Epoch [8/2], Step [9/160], Loss: 0.7339\n",
      "Phase 1 - Epoch [8/2], Step [10/160], Loss: 0.7807\n",
      "Phase 1 - Epoch [8/2], Step [11/160], Loss: 0.9345\n",
      "Phase 1 - Epoch [8/2], Step [12/160], Loss: 0.5189\n",
      "Phase 1 - Epoch [8/2], Step [13/160], Loss: 0.5853\n",
      "Phase 1 - Epoch [8/2], Step [14/160], Loss: 0.7817\n",
      "Phase 1 - Epoch [8/2], Step [15/160], Loss: 0.8174\n",
      "Phase 1 - Epoch [8/2], Step [16/160], Loss: 0.6393\n",
      "Phase 1 - Epoch [8/2], Step [17/160], Loss: 0.7382\n",
      "Phase 1 - Epoch [8/2], Step [18/160], Loss: 0.5826\n",
      "Phase 1 - Epoch [8/2], Step [19/160], Loss: 0.6643\n",
      "Phase 1 - Epoch [8/2], Step [20/160], Loss: 0.6421\n",
      "Phase 1 - Epoch [8/2], Step [21/160], Loss: 0.5519\n",
      "Phase 1 - Epoch [8/2], Step [22/160], Loss: 0.7779\n",
      "Phase 1 - Epoch [8/2], Step [23/160], Loss: 0.8795\n",
      "Phase 1 - Epoch [8/2], Step [24/160], Loss: 0.8356\n",
      "Phase 1 - Epoch [8/2], Step [25/160], Loss: 0.6872\n",
      "Phase 1 - Epoch [8/2], Step [26/160], Loss: 0.7554\n",
      "Phase 1 - Epoch [8/2], Step [27/160], Loss: 0.4701\n",
      "Phase 1 - Epoch [8/2], Step [28/160], Loss: 0.6633\n",
      "Phase 1 - Epoch [8/2], Step [29/160], Loss: 0.7578\n",
      "Phase 1 - Epoch [8/2], Step [30/160], Loss: 0.6743\n",
      "Phase 1 - Epoch [8/2], Step [31/160], Loss: 0.7606\n",
      "Phase 1 - Epoch [8/2], Step [32/160], Loss: 0.6376\n",
      "Phase 1 - Epoch [8/2], Step [33/160], Loss: 0.5198\n",
      "Phase 1 - Epoch [8/2], Step [34/160], Loss: 0.5979\n",
      "Phase 1 - Epoch [8/2], Step [35/160], Loss: 0.6928\n",
      "Phase 1 - Epoch [8/2], Step [36/160], Loss: 0.7682\n",
      "Phase 1 - Epoch [8/2], Step [37/160], Loss: 0.7627\n",
      "Phase 1 - Epoch [8/2], Step [38/160], Loss: 0.6282\n",
      "Phase 1 - Epoch [8/2], Step [39/160], Loss: 0.5640\n",
      "Phase 1 - Epoch [8/2], Step [40/160], Loss: 0.6517\n",
      "Phase 1 - Epoch [8/2], Step [41/160], Loss: 0.6172\n",
      "Phase 1 - Epoch [8/2], Step [42/160], Loss: 0.5722\n",
      "Phase 1 - Epoch [8/2], Step [43/160], Loss: 0.6448\n",
      "Phase 1 - Epoch [8/2], Step [44/160], Loss: 0.4900\n",
      "Phase 1 - Epoch [8/2], Step [45/160], Loss: 0.6050\n",
      "Phase 1 - Epoch [8/2], Step [46/160], Loss: 0.7472\n",
      "Phase 1 - Epoch [8/2], Step [47/160], Loss: 0.5888\n",
      "Phase 1 - Epoch [8/2], Step [48/160], Loss: 0.7429\n",
      "Phase 1 - Epoch [8/2], Step [49/160], Loss: 0.9523\n",
      "Phase 1 - Epoch [8/2], Step [50/160], Loss: 0.5863\n",
      "Phase 1 - Epoch [8/2], Step [51/160], Loss: 0.6040\n",
      "Phase 1 - Epoch [8/2], Step [52/160], Loss: 0.7631\n",
      "Phase 1 - Epoch [8/2], Step [53/160], Loss: 0.7603\n",
      "Phase 1 - Epoch [8/2], Step [54/160], Loss: 0.8703\n",
      "Phase 1 - Epoch [8/2], Step [55/160], Loss: 0.6969\n",
      "Phase 1 - Epoch [8/2], Step [56/160], Loss: 0.6365\n",
      "Phase 1 - Epoch [8/2], Step [57/160], Loss: 0.7683\n",
      "Phase 1 - Epoch [8/2], Step [58/160], Loss: 0.7131\n",
      "Phase 1 - Epoch [8/2], Step [59/160], Loss: 0.8220\n",
      "Phase 1 - Epoch [8/2], Step [60/160], Loss: 0.4416\n",
      "Phase 1 - Epoch [8/2], Step [61/160], Loss: 0.7893\n",
      "Phase 1 - Epoch [8/2], Step [62/160], Loss: 0.7108\n",
      "Phase 1 - Epoch [8/2], Step [63/160], Loss: 0.5656\n",
      "Phase 1 - Epoch [8/2], Step [64/160], Loss: 0.9611\n",
      "Phase 1 - Epoch [8/2], Step [65/160], Loss: 0.9222\n",
      "Phase 1 - Epoch [8/2], Step [66/160], Loss: 0.6648\n",
      "Phase 1 - Epoch [8/2], Step [67/160], Loss: 0.5619\n",
      "Phase 1 - Epoch [8/2], Step [68/160], Loss: 0.7822\n",
      "Phase 1 - Epoch [8/2], Step [69/160], Loss: 0.6888\n",
      "Phase 1 - Epoch [8/2], Step [70/160], Loss: 0.6034\n",
      "Phase 1 - Epoch [8/2], Step [71/160], Loss: 0.7975\n",
      "Phase 1 - Epoch [8/2], Step [72/160], Loss: 0.7392\n",
      "Phase 1 - Epoch [8/2], Step [73/160], Loss: 0.7500\n",
      "Phase 1 - Epoch [8/2], Step [74/160], Loss: 0.8379\n",
      "Phase 1 - Epoch [8/2], Step [75/160], Loss: 0.5725\n",
      "Phase 1 - Epoch [8/2], Step [76/160], Loss: 0.7279\n",
      "Phase 1 - Epoch [8/2], Step [77/160], Loss: 0.7268\n",
      "Phase 1 - Epoch [8/2], Step [78/160], Loss: 0.6605\n",
      "Phase 1 - Epoch [8/2], Step [79/160], Loss: 0.6440\n",
      "Phase 1 - Epoch [8/2], Step [80/160], Loss: 0.6084\n",
      "Phase 1 - Epoch [8/2], Step [81/160], Loss: 0.6562\n",
      "Phase 1 - Epoch [8/2], Step [82/160], Loss: 0.7348\n",
      "Phase 1 - Epoch [8/2], Step [83/160], Loss: 0.5760\n",
      "Phase 1 - Epoch [8/2], Step [84/160], Loss: 0.7683\n",
      "Phase 1 - Epoch [8/2], Step [85/160], Loss: 0.8142\n",
      "Phase 1 - Epoch [8/2], Step [86/160], Loss: 0.5507\n",
      "Phase 1 - Epoch [8/2], Step [87/160], Loss: 0.7561\n",
      "Phase 1 - Epoch [8/2], Step [88/160], Loss: 0.4636\n",
      "Phase 1 - Epoch [8/2], Step [89/160], Loss: 0.6245\n",
      "Phase 1 - Epoch [8/2], Step [90/160], Loss: 0.6282\n",
      "Phase 1 - Epoch [8/2], Step [91/160], Loss: 0.6652\n",
      "Phase 1 - Epoch [8/2], Step [92/160], Loss: 0.5682\n",
      "Phase 1 - Epoch [8/2], Step [93/160], Loss: 0.6570\n",
      "Phase 1 - Epoch [8/2], Step [94/160], Loss: 0.8431\n",
      "Phase 1 - Epoch [8/2], Step [95/160], Loss: 0.5925\n",
      "Phase 1 - Epoch [8/2], Step [96/160], Loss: 0.7519\n",
      "Phase 1 - Epoch [8/2], Step [97/160], Loss: 0.4724\n",
      "Phase 1 - Epoch [8/2], Step [98/160], Loss: 0.6973\n",
      "Phase 1 - Epoch [8/2], Step [99/160], Loss: 0.5911\n",
      "Phase 1 - Epoch [8/2], Step [100/160], Loss: 0.8211\n",
      "Phase 1 - Epoch [8/2], Step [101/160], Loss: 0.5799\n",
      "Phase 1 - Epoch [8/2], Step [102/160], Loss: 0.5526\n",
      "Phase 1 - Epoch [8/2], Step [103/160], Loss: 0.5268\n",
      "Phase 1 - Epoch [8/2], Step [104/160], Loss: 0.5776\n",
      "Phase 1 - Epoch [8/2], Step [105/160], Loss: 0.5219\n",
      "Phase 1 - Epoch [8/2], Step [106/160], Loss: 0.5050\n",
      "Phase 1 - Epoch [8/2], Step [107/160], Loss: 0.7302\n",
      "Phase 1 - Epoch [8/2], Step [108/160], Loss: 0.5736\n",
      "Phase 1 - Epoch [8/2], Step [109/160], Loss: 0.6990\n",
      "Phase 1 - Epoch [8/2], Step [110/160], Loss: 0.4744\n",
      "Phase 1 - Epoch [8/2], Step [111/160], Loss: 0.7265\n",
      "Phase 1 - Epoch [8/2], Step [112/160], Loss: 0.6959\n",
      "Phase 1 - Epoch [8/2], Step [113/160], Loss: 0.5789\n",
      "Phase 1 - Epoch [8/2], Step [114/160], Loss: 1.0029\n",
      "Phase 1 - Epoch [8/2], Step [115/160], Loss: 0.6298\n",
      "Phase 1 - Epoch [8/2], Step [116/160], Loss: 0.6601\n",
      "Phase 1 - Epoch [8/2], Step [117/160], Loss: 0.5867\n",
      "Phase 1 - Epoch [8/2], Step [118/160], Loss: 0.9364\n",
      "Phase 1 - Epoch [8/2], Step [119/160], Loss: 0.6459\n",
      "Phase 1 - Epoch [8/2], Step [120/160], Loss: 0.6808\n",
      "Phase 1 - Epoch [8/2], Step [121/160], Loss: 1.0279\n",
      "Phase 1 - Epoch [8/2], Step [122/160], Loss: 0.7616\n",
      "Phase 1 - Epoch [8/2], Step [123/160], Loss: 0.5572\n",
      "Phase 1 - Epoch [8/2], Step [124/160], Loss: 0.7082\n",
      "Phase 1 - Epoch [8/2], Step [125/160], Loss: 0.8949\n",
      "Phase 1 - Epoch [8/2], Step [126/160], Loss: 0.7315\n",
      "Phase 1 - Epoch [8/2], Step [127/160], Loss: 0.6521\n",
      "Phase 1 - Epoch [8/2], Step [128/160], Loss: 0.6268\n",
      "Phase 1 - Epoch [8/2], Step [129/160], Loss: 0.7334\n",
      "Phase 1 - Epoch [8/2], Step [130/160], Loss: 0.7545\n",
      "Phase 1 - Epoch [8/2], Step [131/160], Loss: 0.6429\n",
      "Phase 1 - Epoch [8/2], Step [132/160], Loss: 0.7509\n",
      "Phase 1 - Epoch [8/2], Step [133/160], Loss: 0.9283\n",
      "Phase 1 - Epoch [8/2], Step [134/160], Loss: 0.5651\n",
      "Phase 1 - Epoch [8/2], Step [135/160], Loss: 0.5914\n",
      "Phase 1 - Epoch [8/2], Step [136/160], Loss: 0.6045\n",
      "Phase 1 - Epoch [8/2], Step [137/160], Loss: 0.6478\n",
      "Phase 1 - Epoch [8/2], Step [138/160], Loss: 0.5143\n",
      "Phase 1 - Epoch [8/2], Step [139/160], Loss: 0.6974\n",
      "Phase 1 - Epoch [8/2], Step [140/160], Loss: 0.6058\n",
      "Phase 1 - Epoch [8/2], Step [141/160], Loss: 0.6638\n",
      "Phase 1 - Epoch [8/2], Step [142/160], Loss: 0.7079\n",
      "Phase 1 - Epoch [8/2], Step [143/160], Loss: 0.7692\n",
      "Phase 1 - Epoch [8/2], Step [144/160], Loss: 0.6981\n",
      "Phase 1 - Epoch [8/2], Step [145/160], Loss: 0.5633\n",
      "Phase 1 - Epoch [8/2], Step [146/160], Loss: 0.5806\n",
      "Phase 1 - Epoch [8/2], Step [147/160], Loss: 0.6292\n",
      "Phase 1 - Epoch [8/2], Step [148/160], Loss: 0.9669\n",
      "Phase 1 - Epoch [8/2], Step [149/160], Loss: 0.7362\n",
      "Phase 1 - Epoch [8/2], Step [150/160], Loss: 0.7386\n",
      "Phase 1 - Epoch [8/2], Step [151/160], Loss: 1.0857\n",
      "Phase 1 - Epoch [8/2], Step [152/160], Loss: 0.4971\n",
      "Phase 1 - Epoch [8/2], Step [153/160], Loss: 0.4743\n",
      "Phase 1 - Epoch [8/2], Step [154/160], Loss: 0.5899\n",
      "Phase 1 - Epoch [8/2], Step [155/160], Loss: 0.6241\n",
      "Phase 1 - Epoch [8/2], Step [156/160], Loss: 0.5822\n",
      "Phase 1 - Epoch [8/2], Step [157/160], Loss: 0.4510\n",
      "Phase 1 - Epoch [8/2], Step [158/160], Loss: 0.6284\n",
      "Phase 1 - Epoch [8/2], Step [159/160], Loss: 0.8949\n",
      "Phase 1 - Epoch [8/2], Step [160/160], Loss: 0.5354\n",
      "Validation Loss: 1.0743\n",
      "Train F1: 0.6960, Train Recall: 0.6953, Train Accuracy: 0.6953\n",
      "Val F1: 0.5244, Val Recall: 0.5379, Val Accuracy: 0.5379\n",
      "Best recall so far: 0.5707584050039093, Best parameters so far: {'dropout': 0.3, 'lr': 0.001}\n",
      "Phase 1 - Epoch [9/2], Step [1/160], Loss: 0.3721\n",
      "Phase 1 - Epoch [9/2], Step [2/160], Loss: 0.8016\n",
      "Phase 1 - Epoch [9/2], Step [3/160], Loss: 0.4998\n",
      "Phase 1 - Epoch [9/2], Step [4/160], Loss: 0.6322\n",
      "Phase 1 - Epoch [9/2], Step [5/160], Loss: 0.7828\n",
      "Phase 1 - Epoch [9/2], Step [6/160], Loss: 0.7730\n",
      "Phase 1 - Epoch [9/2], Step [7/160], Loss: 0.9343\n",
      "Phase 1 - Epoch [9/2], Step [8/160], Loss: 0.8999\n",
      "Phase 1 - Epoch [9/2], Step [9/160], Loss: 0.7209\n",
      "Phase 1 - Epoch [9/2], Step [10/160], Loss: 0.5067\n",
      "Phase 1 - Epoch [9/2], Step [11/160], Loss: 0.6435\n",
      "Phase 1 - Epoch [9/2], Step [12/160], Loss: 0.6420\n",
      "Phase 1 - Epoch [9/2], Step [13/160], Loss: 0.6628\n",
      "Phase 1 - Epoch [9/2], Step [14/160], Loss: 0.6220\n",
      "Phase 1 - Epoch [9/2], Step [15/160], Loss: 0.5706\n",
      "Phase 1 - Epoch [9/2], Step [16/160], Loss: 0.7249\n",
      "Phase 1 - Epoch [9/2], Step [17/160], Loss: 0.7545\n",
      "Phase 1 - Epoch [9/2], Step [18/160], Loss: 0.8188\n",
      "Phase 1 - Epoch [9/2], Step [19/160], Loss: 0.5818\n",
      "Phase 1 - Epoch [9/2], Step [20/160], Loss: 0.6475\n",
      "Phase 1 - Epoch [9/2], Step [21/160], Loss: 0.6935\n",
      "Phase 1 - Epoch [9/2], Step [22/160], Loss: 0.6476\n",
      "Phase 1 - Epoch [9/2], Step [23/160], Loss: 0.5816\n",
      "Phase 1 - Epoch [9/2], Step [24/160], Loss: 0.6811\n",
      "Phase 1 - Epoch [9/2], Step [25/160], Loss: 0.6740\n",
      "Phase 1 - Epoch [9/2], Step [26/160], Loss: 0.4756\n",
      "Phase 1 - Epoch [9/2], Step [27/160], Loss: 0.5216\n",
      "Phase 1 - Epoch [9/2], Step [28/160], Loss: 0.7375\n",
      "Phase 1 - Epoch [9/2], Step [29/160], Loss: 0.6760\n",
      "Phase 1 - Epoch [9/2], Step [30/160], Loss: 0.5345\n",
      "Phase 1 - Epoch [9/2], Step [31/160], Loss: 0.8073\n",
      "Phase 1 - Epoch [9/2], Step [32/160], Loss: 0.5916\n",
      "Phase 1 - Epoch [9/2], Step [33/160], Loss: 0.7135\n",
      "Phase 1 - Epoch [9/2], Step [34/160], Loss: 0.5693\n",
      "Phase 1 - Epoch [9/2], Step [35/160], Loss: 0.7041\n",
      "Phase 1 - Epoch [9/2], Step [36/160], Loss: 0.5658\n",
      "Phase 1 - Epoch [9/2], Step [37/160], Loss: 0.6645\n",
      "Phase 1 - Epoch [9/2], Step [38/160], Loss: 0.5524\n",
      "Phase 1 - Epoch [9/2], Step [39/160], Loss: 0.5130\n",
      "Phase 1 - Epoch [9/2], Step [40/160], Loss: 0.7012\n",
      "Phase 1 - Epoch [9/2], Step [41/160], Loss: 0.5288\n",
      "Phase 1 - Epoch [9/2], Step [42/160], Loss: 0.5572\n",
      "Phase 1 - Epoch [9/2], Step [43/160], Loss: 0.4413\n",
      "Phase 1 - Epoch [9/2], Step [44/160], Loss: 0.5109\n",
      "Phase 1 - Epoch [9/2], Step [45/160], Loss: 0.5781\n",
      "Phase 1 - Epoch [9/2], Step [46/160], Loss: 0.6318\n",
      "Phase 1 - Epoch [9/2], Step [47/160], Loss: 0.2653\n",
      "Phase 1 - Epoch [9/2], Step [48/160], Loss: 0.5950\n",
      "Phase 1 - Epoch [9/2], Step [49/160], Loss: 0.7417\n",
      "Phase 1 - Epoch [9/2], Step [50/160], Loss: 0.6544\n",
      "Phase 1 - Epoch [9/2], Step [51/160], Loss: 0.7703\n",
      "Phase 1 - Epoch [9/2], Step [52/160], Loss: 0.8074\n",
      "Phase 1 - Epoch [9/2], Step [53/160], Loss: 0.5482\n",
      "Phase 1 - Epoch [9/2], Step [54/160], Loss: 0.6523\n",
      "Phase 1 - Epoch [9/2], Step [55/160], Loss: 0.5501\n",
      "Phase 1 - Epoch [9/2], Step [56/160], Loss: 0.5258\n",
      "Phase 1 - Epoch [9/2], Step [57/160], Loss: 0.4845\n",
      "Phase 1 - Epoch [9/2], Step [58/160], Loss: 0.9845\n",
      "Phase 1 - Epoch [9/2], Step [59/160], Loss: 0.6479\n",
      "Phase 1 - Epoch [9/2], Step [60/160], Loss: 0.4300\n",
      "Phase 1 - Epoch [9/2], Step [61/160], Loss: 0.4902\n",
      "Phase 1 - Epoch [9/2], Step [62/160], Loss: 0.6302\n",
      "Phase 1 - Epoch [9/2], Step [63/160], Loss: 0.7833\n",
      "Phase 1 - Epoch [9/2], Step [64/160], Loss: 0.5960\n",
      "Phase 1 - Epoch [9/2], Step [65/160], Loss: 0.7061\n",
      "Phase 1 - Epoch [9/2], Step [66/160], Loss: 0.5805\n",
      "Phase 1 - Epoch [9/2], Step [67/160], Loss: 0.4485\n",
      "Phase 1 - Epoch [9/2], Step [68/160], Loss: 0.7769\n",
      "Phase 1 - Epoch [9/2], Step [69/160], Loss: 0.7995\n",
      "Phase 1 - Epoch [9/2], Step [70/160], Loss: 0.5918\n",
      "Phase 1 - Epoch [9/2], Step [71/160], Loss: 0.5370\n",
      "Phase 1 - Epoch [9/2], Step [72/160], Loss: 0.5590\n",
      "Phase 1 - Epoch [9/2], Step [73/160], Loss: 0.6861\n",
      "Phase 1 - Epoch [9/2], Step [74/160], Loss: 0.7258\n",
      "Phase 1 - Epoch [9/2], Step [75/160], Loss: 0.8572\n",
      "Phase 1 - Epoch [9/2], Step [76/160], Loss: 0.9573\n",
      "Phase 1 - Epoch [9/2], Step [77/160], Loss: 0.5943\n",
      "Phase 1 - Epoch [9/2], Step [78/160], Loss: 0.5268\n",
      "Phase 1 - Epoch [9/2], Step [79/160], Loss: 0.4534\n",
      "Phase 1 - Epoch [9/2], Step [80/160], Loss: 0.8161\n",
      "Phase 1 - Epoch [9/2], Step [81/160], Loss: 0.5592\n",
      "Phase 1 - Epoch [9/2], Step [82/160], Loss: 0.6136\n",
      "Phase 1 - Epoch [9/2], Step [83/160], Loss: 0.6134\n",
      "Phase 1 - Epoch [9/2], Step [84/160], Loss: 0.7780\n",
      "Phase 1 - Epoch [9/2], Step [85/160], Loss: 0.6567\n",
      "Phase 1 - Epoch [9/2], Step [86/160], Loss: 0.8111\n",
      "Phase 1 - Epoch [9/2], Step [87/160], Loss: 0.6851\n",
      "Phase 1 - Epoch [9/2], Step [88/160], Loss: 0.6508\n",
      "Phase 1 - Epoch [9/2], Step [89/160], Loss: 0.6536\n",
      "Phase 1 - Epoch [9/2], Step [90/160], Loss: 0.4163\n",
      "Phase 1 - Epoch [9/2], Step [91/160], Loss: 0.5833\n",
      "Phase 1 - Epoch [9/2], Step [92/160], Loss: 0.8028\n",
      "Phase 1 - Epoch [9/2], Step [93/160], Loss: 0.7011\n",
      "Phase 1 - Epoch [9/2], Step [94/160], Loss: 0.7970\n",
      "Phase 1 - Epoch [9/2], Step [95/160], Loss: 0.5887\n",
      "Phase 1 - Epoch [9/2], Step [96/160], Loss: 0.4789\n",
      "Phase 1 - Epoch [9/2], Step [97/160], Loss: 0.6774\n",
      "Phase 1 - Epoch [9/2], Step [98/160], Loss: 0.6096\n",
      "Phase 1 - Epoch [9/2], Step [99/160], Loss: 0.5675\n",
      "Phase 1 - Epoch [9/2], Step [100/160], Loss: 0.6446\n",
      "Phase 1 - Epoch [9/2], Step [101/160], Loss: 0.6265\n",
      "Phase 1 - Epoch [9/2], Step [102/160], Loss: 0.6178\n",
      "Phase 1 - Epoch [9/2], Step [103/160], Loss: 0.7613\n",
      "Phase 1 - Epoch [9/2], Step [104/160], Loss: 0.9393\n",
      "Phase 1 - Epoch [9/2], Step [105/160], Loss: 0.5016\n",
      "Phase 1 - Epoch [9/2], Step [106/160], Loss: 0.7867\n",
      "Phase 1 - Epoch [9/2], Step [107/160], Loss: 0.5689\n",
      "Phase 1 - Epoch [9/2], Step [108/160], Loss: 0.6959\n",
      "Phase 1 - Epoch [9/2], Step [109/160], Loss: 0.4520\n",
      "Phase 1 - Epoch [9/2], Step [110/160], Loss: 0.6209\n",
      "Phase 1 - Epoch [9/2], Step [111/160], Loss: 0.7542\n",
      "Phase 1 - Epoch [9/2], Step [112/160], Loss: 0.4616\n",
      "Phase 1 - Epoch [9/2], Step [113/160], Loss: 0.4396\n",
      "Phase 1 - Epoch [9/2], Step [114/160], Loss: 0.6396\n",
      "Phase 1 - Epoch [9/2], Step [115/160], Loss: 0.7415\n",
      "Phase 1 - Epoch [9/2], Step [116/160], Loss: 0.7201\n",
      "Phase 1 - Epoch [9/2], Step [117/160], Loss: 0.4622\n",
      "Phase 1 - Epoch [9/2], Step [118/160], Loss: 0.6095\n",
      "Phase 1 - Epoch [9/2], Step [119/160], Loss: 0.6193\n",
      "Phase 1 - Epoch [9/2], Step [120/160], Loss: 0.5744\n",
      "Phase 1 - Epoch [9/2], Step [121/160], Loss: 0.5284\n",
      "Phase 1 - Epoch [9/2], Step [122/160], Loss: 0.4962\n",
      "Phase 1 - Epoch [9/2], Step [123/160], Loss: 0.6412\n",
      "Phase 1 - Epoch [9/2], Step [124/160], Loss: 0.8118\n",
      "Phase 1 - Epoch [9/2], Step [125/160], Loss: 0.7179\n",
      "Phase 1 - Epoch [9/2], Step [126/160], Loss: 0.6948\n",
      "Phase 1 - Epoch [9/2], Step [127/160], Loss: 0.4239\n",
      "Phase 1 - Epoch [9/2], Step [128/160], Loss: 0.6121\n",
      "Phase 1 - Epoch [9/2], Step [129/160], Loss: 0.4138\n",
      "Phase 1 - Epoch [9/2], Step [130/160], Loss: 0.7475\n",
      "Phase 1 - Epoch [9/2], Step [131/160], Loss: 0.6911\n",
      "Phase 1 - Epoch [9/2], Step [132/160], Loss: 0.5862\n",
      "Phase 1 - Epoch [9/2], Step [133/160], Loss: 0.5651\n",
      "Phase 1 - Epoch [9/2], Step [134/160], Loss: 0.4266\n",
      "Phase 1 - Epoch [9/2], Step [135/160], Loss: 0.7459\n",
      "Phase 1 - Epoch [9/2], Step [136/160], Loss: 0.7033\n",
      "Phase 1 - Epoch [9/2], Step [137/160], Loss: 1.0153\n",
      "Phase 1 - Epoch [9/2], Step [138/160], Loss: 0.4223\n",
      "Phase 1 - Epoch [9/2], Step [139/160], Loss: 0.8799\n",
      "Phase 1 - Epoch [9/2], Step [140/160], Loss: 0.5641\n",
      "Phase 1 - Epoch [9/2], Step [141/160], Loss: 0.3988\n",
      "Phase 1 - Epoch [9/2], Step [142/160], Loss: 0.5797\n",
      "Phase 1 - Epoch [9/2], Step [143/160], Loss: 0.7795\n",
      "Phase 1 - Epoch [9/2], Step [144/160], Loss: 0.6467\n",
      "Phase 1 - Epoch [9/2], Step [145/160], Loss: 0.7523\n",
      "Phase 1 - Epoch [9/2], Step [146/160], Loss: 0.6892\n",
      "Phase 1 - Epoch [9/2], Step [147/160], Loss: 0.6992\n",
      "Phase 1 - Epoch [9/2], Step [148/160], Loss: 0.7593\n",
      "Phase 1 - Epoch [9/2], Step [149/160], Loss: 0.4823\n",
      "Phase 1 - Epoch [9/2], Step [150/160], Loss: 0.4769\n",
      "Phase 1 - Epoch [9/2], Step [151/160], Loss: 0.6133\n",
      "Phase 1 - Epoch [9/2], Step [152/160], Loss: 0.6613\n",
      "Phase 1 - Epoch [9/2], Step [153/160], Loss: 0.8343\n",
      "Phase 1 - Epoch [9/2], Step [154/160], Loss: 0.6068\n",
      "Phase 1 - Epoch [9/2], Step [155/160], Loss: 0.4844\n",
      "Phase 1 - Epoch [9/2], Step [156/160], Loss: 0.6304\n",
      "Phase 1 - Epoch [9/2], Step [157/160], Loss: 0.6688\n",
      "Phase 1 - Epoch [9/2], Step [158/160], Loss: 0.7426\n",
      "Phase 1 - Epoch [9/2], Step [159/160], Loss: 0.8869\n",
      "Phase 1 - Epoch [9/2], Step [160/160], Loss: 0.8313\n",
      "Validation Loss: 0.9794\n",
      "Train F1: 0.7334, Train Recall: 0.7330, Train Accuracy: 0.7330\n",
      "Val F1: 0.5779, Val Recall: 0.5778, Val Accuracy: 0.5778\n",
      "Best recall so far: 0.5777951524628616, Best parameters so far: {'dropout': 0.3, 'lr': 0.001}\n",
      "Early stopping in Phase 1.\n",
      "Phase 2 - Epoch [1/2], Step [1/160], Loss: 0.5300\n",
      "Phase 2 - Epoch [1/2], Step [2/160], Loss: 0.4845\n",
      "Phase 2 - Epoch [1/2], Step [3/160], Loss: 0.5629\n",
      "Phase 2 - Epoch [1/2], Step [4/160], Loss: 0.6734\n",
      "Phase 2 - Epoch [1/2], Step [5/160], Loss: 0.4354\n",
      "Phase 2 - Epoch [1/2], Step [6/160], Loss: 0.3082\n",
      "Phase 2 - Epoch [1/2], Step [7/160], Loss: 0.4731\n",
      "Phase 2 - Epoch [1/2], Step [8/160], Loss: 0.5220\n",
      "Phase 2 - Epoch [1/2], Step [9/160], Loss: 0.5888\n",
      "Phase 2 - Epoch [1/2], Step [10/160], Loss: 0.8803\n",
      "Phase 2 - Epoch [1/2], Step [11/160], Loss: 0.5382\n",
      "Phase 2 - Epoch [1/2], Step [12/160], Loss: 0.5356\n",
      "Phase 2 - Epoch [1/2], Step [13/160], Loss: 0.7295\n",
      "Phase 2 - Epoch [1/2], Step [14/160], Loss: 0.6890\n",
      "Phase 2 - Epoch [1/2], Step [15/160], Loss: 0.5271\n",
      "Phase 2 - Epoch [1/2], Step [16/160], Loss: 0.4721\n",
      "Phase 2 - Epoch [1/2], Step [17/160], Loss: 0.6306\n",
      "Phase 2 - Epoch [1/2], Step [18/160], Loss: 0.7905\n",
      "Phase 2 - Epoch [1/2], Step [19/160], Loss: 0.8150\n",
      "Phase 2 - Epoch [1/2], Step [20/160], Loss: 0.4234\n",
      "Phase 2 - Epoch [1/2], Step [21/160], Loss: 0.4840\n",
      "Phase 2 - Epoch [1/2], Step [22/160], Loss: 0.3894\n",
      "Phase 2 - Epoch [1/2], Step [23/160], Loss: 0.4914\n",
      "Phase 2 - Epoch [1/2], Step [24/160], Loss: 0.4321\n",
      "Phase 2 - Epoch [1/2], Step [25/160], Loss: 0.8121\n",
      "Phase 2 - Epoch [1/2], Step [26/160], Loss: 0.3723\n",
      "Phase 2 - Epoch [1/2], Step [27/160], Loss: 0.6412\n",
      "Phase 2 - Epoch [1/2], Step [28/160], Loss: 0.3500\n",
      "Phase 2 - Epoch [1/2], Step [29/160], Loss: 0.6568\n",
      "Phase 2 - Epoch [1/2], Step [30/160], Loss: 1.0236\n",
      "Phase 2 - Epoch [1/2], Step [31/160], Loss: 0.6666\n",
      "Phase 2 - Epoch [1/2], Step [32/160], Loss: 0.5508\n",
      "Phase 2 - Epoch [1/2], Step [33/160], Loss: 0.3813\n",
      "Phase 2 - Epoch [1/2], Step [34/160], Loss: 0.5120\n",
      "Phase 2 - Epoch [1/2], Step [35/160], Loss: 0.6556\n",
      "Phase 2 - Epoch [1/2], Step [36/160], Loss: 1.0087\n",
      "Phase 2 - Epoch [1/2], Step [37/160], Loss: 0.5973\n",
      "Phase 2 - Epoch [1/2], Step [38/160], Loss: 0.6065\n",
      "Phase 2 - Epoch [1/2], Step [39/160], Loss: 0.4761\n",
      "Phase 2 - Epoch [1/2], Step [40/160], Loss: 0.4528\n",
      "Phase 2 - Epoch [1/2], Step [41/160], Loss: 0.4500\n",
      "Phase 2 - Epoch [1/2], Step [42/160], Loss: 0.8141\n",
      "Phase 2 - Epoch [1/2], Step [43/160], Loss: 0.4422\n",
      "Phase 2 - Epoch [1/2], Step [44/160], Loss: 0.5855\n",
      "Phase 2 - Epoch [1/2], Step [45/160], Loss: 0.5287\n",
      "Phase 2 - Epoch [1/2], Step [46/160], Loss: 0.5841\n",
      "Phase 2 - Epoch [1/2], Step [47/160], Loss: 0.3868\n",
      "Phase 2 - Epoch [1/2], Step [48/160], Loss: 0.3745\n",
      "Phase 2 - Epoch [1/2], Step [49/160], Loss: 0.3816\n",
      "Phase 2 - Epoch [1/2], Step [50/160], Loss: 0.3625\n",
      "Phase 2 - Epoch [1/2], Step [51/160], Loss: 0.4520\n",
      "Phase 2 - Epoch [1/2], Step [52/160], Loss: 0.4944\n",
      "Phase 2 - Epoch [1/2], Step [53/160], Loss: 0.4663\n",
      "Phase 2 - Epoch [1/2], Step [54/160], Loss: 0.5194\n",
      "Phase 2 - Epoch [1/2], Step [55/160], Loss: 0.4450\n",
      "Phase 2 - Epoch [1/2], Step [56/160], Loss: 0.6893\n",
      "Phase 2 - Epoch [1/2], Step [57/160], Loss: 0.2528\n",
      "Phase 2 - Epoch [1/2], Step [58/160], Loss: 0.4627\n",
      "Phase 2 - Epoch [1/2], Step [59/160], Loss: 0.6750\n",
      "Phase 2 - Epoch [1/2], Step [60/160], Loss: 0.7667\n",
      "Phase 2 - Epoch [1/2], Step [61/160], Loss: 0.5646\n",
      "Phase 2 - Epoch [1/2], Step [62/160], Loss: 0.5994\n",
      "Phase 2 - Epoch [1/2], Step [63/160], Loss: 0.5506\n",
      "Phase 2 - Epoch [1/2], Step [64/160], Loss: 0.3973\n",
      "Phase 2 - Epoch [1/2], Step [65/160], Loss: 0.4844\n",
      "Phase 2 - Epoch [1/2], Step [66/160], Loss: 0.7519\n",
      "Phase 2 - Epoch [1/2], Step [67/160], Loss: 0.3460\n",
      "Phase 2 - Epoch [1/2], Step [68/160], Loss: 0.3402\n",
      "Phase 2 - Epoch [1/2], Step [69/160], Loss: 0.6513\n",
      "Phase 2 - Epoch [1/2], Step [70/160], Loss: 0.7399\n",
      "Phase 2 - Epoch [1/2], Step [71/160], Loss: 0.4435\n",
      "Phase 2 - Epoch [1/2], Step [72/160], Loss: 0.3377\n",
      "Phase 2 - Epoch [1/2], Step [73/160], Loss: 0.7489\n",
      "Phase 2 - Epoch [1/2], Step [74/160], Loss: 0.5078\n",
      "Phase 2 - Epoch [1/2], Step [75/160], Loss: 0.5164\n",
      "Phase 2 - Epoch [1/2], Step [76/160], Loss: 0.6298\n",
      "Phase 2 - Epoch [1/2], Step [77/160], Loss: 0.9817\n",
      "Phase 2 - Epoch [1/2], Step [78/160], Loss: 0.4651\n",
      "Phase 2 - Epoch [1/2], Step [79/160], Loss: 0.7365\n",
      "Phase 2 - Epoch [1/2], Step [80/160], Loss: 0.6022\n",
      "Phase 2 - Epoch [1/2], Step [81/160], Loss: 0.7226\n",
      "Phase 2 - Epoch [1/2], Step [82/160], Loss: 0.7165\n",
      "Phase 2 - Epoch [1/2], Step [83/160], Loss: 0.7836\n",
      "Phase 2 - Epoch [1/2], Step [84/160], Loss: 0.4600\n",
      "Phase 2 - Epoch [1/2], Step [85/160], Loss: 0.3596\n",
      "Phase 2 - Epoch [1/2], Step [86/160], Loss: 0.4192\n",
      "Phase 2 - Epoch [1/2], Step [87/160], Loss: 0.5233\n",
      "Phase 2 - Epoch [1/2], Step [88/160], Loss: 0.7028\n",
      "Phase 2 - Epoch [1/2], Step [89/160], Loss: 0.4834\n",
      "Phase 2 - Epoch [1/2], Step [90/160], Loss: 0.7494\n",
      "Phase 2 - Epoch [1/2], Step [91/160], Loss: 0.8476\n",
      "Phase 2 - Epoch [1/2], Step [92/160], Loss: 0.4781\n",
      "Phase 2 - Epoch [1/2], Step [93/160], Loss: 0.4409\n",
      "Phase 2 - Epoch [1/2], Step [94/160], Loss: 0.5779\n",
      "Phase 2 - Epoch [1/2], Step [95/160], Loss: 0.4441\n",
      "Phase 2 - Epoch [1/2], Step [96/160], Loss: 0.5333\n",
      "Phase 2 - Epoch [1/2], Step [97/160], Loss: 0.5887\n",
      "Phase 2 - Epoch [1/2], Step [98/160], Loss: 0.9906\n",
      "Phase 2 - Epoch [1/2], Step [99/160], Loss: 0.5584\n",
      "Phase 2 - Epoch [1/2], Step [100/160], Loss: 0.5954\n",
      "Phase 2 - Epoch [1/2], Step [101/160], Loss: 0.7805\n",
      "Phase 2 - Epoch [1/2], Step [102/160], Loss: 0.5447\n",
      "Phase 2 - Epoch [1/2], Step [103/160], Loss: 0.7051\n",
      "Phase 2 - Epoch [1/2], Step [104/160], Loss: 0.7121\n",
      "Phase 2 - Epoch [1/2], Step [105/160], Loss: 0.4914\n",
      "Phase 2 - Epoch [1/2], Step [106/160], Loss: 0.6434\n",
      "Phase 2 - Epoch [1/2], Step [107/160], Loss: 0.5267\n",
      "Phase 2 - Epoch [1/2], Step [108/160], Loss: 0.4886\n",
      "Phase 2 - Epoch [1/2], Step [109/160], Loss: 0.7405\n",
      "Phase 2 - Epoch [1/2], Step [110/160], Loss: 0.5792\n",
      "Phase 2 - Epoch [1/2], Step [111/160], Loss: 0.5706\n",
      "Phase 2 - Epoch [1/2], Step [112/160], Loss: 0.5212\n",
      "Phase 2 - Epoch [1/2], Step [113/160], Loss: 0.6292\n",
      "Phase 2 - Epoch [1/2], Step [114/160], Loss: 0.7389\n",
      "Phase 2 - Epoch [1/2], Step [115/160], Loss: 0.5063\n",
      "Phase 2 - Epoch [1/2], Step [116/160], Loss: 0.5418\n",
      "Phase 2 - Epoch [1/2], Step [117/160], Loss: 0.4820\n",
      "Phase 2 - Epoch [1/2], Step [118/160], Loss: 0.4948\n",
      "Phase 2 - Epoch [1/2], Step [119/160], Loss: 0.7584\n",
      "Phase 2 - Epoch [1/2], Step [120/160], Loss: 0.5394\n",
      "Phase 2 - Epoch [1/2], Step [121/160], Loss: 0.7245\n",
      "Phase 2 - Epoch [1/2], Step [122/160], Loss: 0.4539\n",
      "Phase 2 - Epoch [1/2], Step [123/160], Loss: 0.7416\n",
      "Phase 2 - Epoch [1/2], Step [124/160], Loss: 0.6489\n",
      "Phase 2 - Epoch [1/2], Step [125/160], Loss: 0.7426\n",
      "Phase 2 - Epoch [1/2], Step [126/160], Loss: 0.8510\n",
      "Phase 2 - Epoch [1/2], Step [127/160], Loss: 0.5039\n",
      "Phase 2 - Epoch [1/2], Step [128/160], Loss: 0.6272\n",
      "Phase 2 - Epoch [1/2], Step [129/160], Loss: 0.5286\n",
      "Phase 2 - Epoch [1/2], Step [130/160], Loss: 0.4815\n",
      "Phase 2 - Epoch [1/2], Step [131/160], Loss: 0.5691\n",
      "Phase 2 - Epoch [1/2], Step [132/160], Loss: 0.8708\n",
      "Phase 2 - Epoch [1/2], Step [133/160], Loss: 0.7404\n",
      "Phase 2 - Epoch [1/2], Step [134/160], Loss: 0.6090\n",
      "Phase 2 - Epoch [1/2], Step [135/160], Loss: 0.4738\n",
      "Phase 2 - Epoch [1/2], Step [136/160], Loss: 0.6020\n",
      "Phase 2 - Epoch [1/2], Step [137/160], Loss: 0.6094\n",
      "Phase 2 - Epoch [1/2], Step [138/160], Loss: 0.5285\n",
      "Phase 2 - Epoch [1/2], Step [139/160], Loss: 0.5956\n",
      "Phase 2 - Epoch [1/2], Step [140/160], Loss: 0.3486\n",
      "Phase 2 - Epoch [1/2], Step [141/160], Loss: 0.6011\n",
      "Phase 2 - Epoch [1/2], Step [142/160], Loss: 0.6578\n",
      "Phase 2 - Epoch [1/2], Step [143/160], Loss: 0.3844\n",
      "Phase 2 - Epoch [1/2], Step [144/160], Loss: 0.4127\n",
      "Phase 2 - Epoch [1/2], Step [145/160], Loss: 0.6750\n",
      "Phase 2 - Epoch [1/2], Step [146/160], Loss: 0.4998\n",
      "Phase 2 - Epoch [1/2], Step [147/160], Loss: 0.6031\n",
      "Phase 2 - Epoch [1/2], Step [148/160], Loss: 0.5540\n",
      "Phase 2 - Epoch [1/2], Step [149/160], Loss: 0.7553\n",
      "Phase 2 - Epoch [1/2], Step [150/160], Loss: 0.5250\n",
      "Phase 2 - Epoch [1/2], Step [151/160], Loss: 0.5220\n",
      "Phase 2 - Epoch [1/2], Step [152/160], Loss: 0.4126\n",
      "Phase 2 - Epoch [1/2], Step [153/160], Loss: 0.9543\n",
      "Phase 2 - Epoch [1/2], Step [154/160], Loss: 0.6571\n",
      "Phase 2 - Epoch [1/2], Step [155/160], Loss: 0.5974\n",
      "Phase 2 - Epoch [1/2], Step [156/160], Loss: 0.5703\n",
      "Phase 2 - Epoch [1/2], Step [157/160], Loss: 0.5039\n",
      "Phase 2 - Epoch [1/2], Step [158/160], Loss: 0.6092\n",
      "Phase 2 - Epoch [1/2], Step [159/160], Loss: 0.9594\n",
      "Phase 2 - Epoch [1/2], Step [160/160], Loss: 0.3315\n",
      "Validation Loss: 1.0235\n",
      "Train F1: 0.7627, Train Recall: 0.7624, Train Accuracy: 0.7624\n",
      "Val F1: 0.5721, Val Recall: 0.5723, Val Accuracy: 0.5723\n",
      "Early stopping in Phase 2.\n",
      "Training complete.\n",
      "Best recall so far: 0.5777951524628616, Best parameters so far: {'dropout': 0.3, 'lr': 0.001}\n",
      "Training with parameters: {'dropout': 0.3, 'lr': 0.0005}\n",
      "Phase 1 - Epoch [1/2], Step [1/160], Loss: 1.1054\n",
      "Phase 1 - Epoch [1/2], Step [2/160], Loss: 1.0571\n",
      "Phase 1 - Epoch [1/2], Step [3/160], Loss: 1.1906\n",
      "Phase 1 - Epoch [1/2], Step [4/160], Loss: 1.0938\n",
      "Phase 1 - Epoch [1/2], Step [5/160], Loss: 1.0769\n",
      "Phase 1 - Epoch [1/2], Step [6/160], Loss: 1.0204\n",
      "Phase 1 - Epoch [1/2], Step [7/160], Loss: 1.2051\n",
      "Phase 1 - Epoch [1/2], Step [8/160], Loss: 1.0929\n",
      "Phase 1 - Epoch [1/2], Step [9/160], Loss: 1.1030\n",
      "Phase 1 - Epoch [1/2], Step [10/160], Loss: 1.1112\n",
      "Phase 1 - Epoch [1/2], Step [11/160], Loss: 1.1312\n",
      "Phase 1 - Epoch [1/2], Step [12/160], Loss: 1.1208\n",
      "Phase 1 - Epoch [1/2], Step [13/160], Loss: 1.1275\n",
      "Phase 1 - Epoch [1/2], Step [14/160], Loss: 1.0681\n",
      "Phase 1 - Epoch [1/2], Step [15/160], Loss: 0.9714\n",
      "Phase 1 - Epoch [1/2], Step [16/160], Loss: 1.1457\n",
      "Phase 1 - Epoch [1/2], Step [17/160], Loss: 1.0478\n",
      "Phase 1 - Epoch [1/2], Step [18/160], Loss: 1.0152\n",
      "Phase 1 - Epoch [1/2], Step [19/160], Loss: 1.1228\n",
      "Phase 1 - Epoch [1/2], Step [20/160], Loss: 1.0646\n",
      "Phase 1 - Epoch [1/2], Step [21/160], Loss: 1.0048\n",
      "Phase 1 - Epoch [1/2], Step [22/160], Loss: 1.2089\n",
      "Phase 1 - Epoch [1/2], Step [23/160], Loss: 1.1493\n",
      "Phase 1 - Epoch [1/2], Step [24/160], Loss: 1.1317\n",
      "Phase 1 - Epoch [1/2], Step [25/160], Loss: 1.1647\n",
      "Phase 1 - Epoch [1/2], Step [26/160], Loss: 1.0290\n",
      "Phase 1 - Epoch [1/2], Step [27/160], Loss: 1.1259\n",
      "Phase 1 - Epoch [1/2], Step [28/160], Loss: 1.1310\n",
      "Phase 1 - Epoch [1/2], Step [29/160], Loss: 1.2114\n",
      "Phase 1 - Epoch [1/2], Step [30/160], Loss: 1.2462\n",
      "Phase 1 - Epoch [1/2], Step [31/160], Loss: 1.2262\n",
      "Phase 1 - Epoch [1/2], Step [32/160], Loss: 1.1437\n",
      "Phase 1 - Epoch [1/2], Step [33/160], Loss: 1.0879\n",
      "Phase 1 - Epoch [1/2], Step [34/160], Loss: 1.0845\n",
      "Phase 1 - Epoch [1/2], Step [35/160], Loss: 1.0393\n",
      "Phase 1 - Epoch [1/2], Step [36/160], Loss: 1.0548\n",
      "Phase 1 - Epoch [1/2], Step [37/160], Loss: 1.0449\n",
      "Phase 1 - Epoch [1/2], Step [38/160], Loss: 1.0850\n",
      "Phase 1 - Epoch [1/2], Step [39/160], Loss: 1.1339\n",
      "Phase 1 - Epoch [1/2], Step [40/160], Loss: 1.0398\n",
      "Phase 1 - Epoch [1/2], Step [41/160], Loss: 1.0676\n",
      "Phase 1 - Epoch [1/2], Step [42/160], Loss: 0.9976\n",
      "Phase 1 - Epoch [1/2], Step [43/160], Loss: 1.0404\n",
      "Phase 1 - Epoch [1/2], Step [44/160], Loss: 0.9788\n",
      "Phase 1 - Epoch [1/2], Step [45/160], Loss: 0.9879\n",
      "Phase 1 - Epoch [1/2], Step [46/160], Loss: 0.9309\n",
      "Phase 1 - Epoch [1/2], Step [47/160], Loss: 1.0724\n",
      "Phase 1 - Epoch [1/2], Step [48/160], Loss: 1.1206\n",
      "Phase 1 - Epoch [1/2], Step [49/160], Loss: 1.1463\n",
      "Phase 1 - Epoch [1/2], Step [50/160], Loss: 1.0183\n",
      "Phase 1 - Epoch [1/2], Step [51/160], Loss: 1.1261\n",
      "Phase 1 - Epoch [1/2], Step [52/160], Loss: 1.0204\n",
      "Phase 1 - Epoch [1/2], Step [53/160], Loss: 1.0502\n",
      "Phase 1 - Epoch [1/2], Step [54/160], Loss: 1.0298\n",
      "Phase 1 - Epoch [1/2], Step [55/160], Loss: 1.0714\n",
      "Phase 1 - Epoch [1/2], Step [56/160], Loss: 1.1519\n",
      "Phase 1 - Epoch [1/2], Step [57/160], Loss: 1.0478\n",
      "Phase 1 - Epoch [1/2], Step [58/160], Loss: 0.9975\n",
      "Phase 1 - Epoch [1/2], Step [59/160], Loss: 1.1016\n",
      "Phase 1 - Epoch [1/2], Step [60/160], Loss: 1.1767\n",
      "Phase 1 - Epoch [1/2], Step [61/160], Loss: 1.0018\n",
      "Phase 1 - Epoch [1/2], Step [62/160], Loss: 1.1743\n",
      "Phase 1 - Epoch [1/2], Step [63/160], Loss: 1.0336\n",
      "Phase 1 - Epoch [1/2], Step [64/160], Loss: 1.0206\n",
      "Phase 1 - Epoch [1/2], Step [65/160], Loss: 1.1794\n",
      "Phase 1 - Epoch [1/2], Step [66/160], Loss: 0.9672\n",
      "Phase 1 - Epoch [1/2], Step [67/160], Loss: 1.1526\n",
      "Phase 1 - Epoch [1/2], Step [68/160], Loss: 1.0580\n",
      "Phase 1 - Epoch [1/2], Step [69/160], Loss: 1.0396\n",
      "Phase 1 - Epoch [1/2], Step [70/160], Loss: 0.9527\n",
      "Phase 1 - Epoch [1/2], Step [71/160], Loss: 1.0883\n",
      "Phase 1 - Epoch [1/2], Step [72/160], Loss: 1.1839\n",
      "Phase 1 - Epoch [1/2], Step [73/160], Loss: 1.1723\n",
      "Phase 1 - Epoch [1/2], Step [74/160], Loss: 1.1279\n",
      "Phase 1 - Epoch [1/2], Step [75/160], Loss: 1.1035\n",
      "Phase 1 - Epoch [1/2], Step [76/160], Loss: 1.0922\n",
      "Phase 1 - Epoch [1/2], Step [77/160], Loss: 1.0822\n",
      "Phase 1 - Epoch [1/2], Step [78/160], Loss: 1.0242\n",
      "Phase 1 - Epoch [1/2], Step [79/160], Loss: 1.1292\n",
      "Phase 1 - Epoch [1/2], Step [80/160], Loss: 0.9972\n",
      "Phase 1 - Epoch [1/2], Step [81/160], Loss: 1.2039\n",
      "Phase 1 - Epoch [1/2], Step [82/160], Loss: 1.0245\n",
      "Phase 1 - Epoch [1/2], Step [83/160], Loss: 1.0998\n",
      "Phase 1 - Epoch [1/2], Step [84/160], Loss: 1.0514\n",
      "Phase 1 - Epoch [1/2], Step [85/160], Loss: 1.1017\n",
      "Phase 1 - Epoch [1/2], Step [86/160], Loss: 1.0460\n",
      "Phase 1 - Epoch [1/2], Step [87/160], Loss: 1.1115\n",
      "Phase 1 - Epoch [1/2], Step [88/160], Loss: 1.0974\n",
      "Phase 1 - Epoch [1/2], Step [89/160], Loss: 1.0119\n",
      "Phase 1 - Epoch [1/2], Step [90/160], Loss: 1.0347\n",
      "Phase 1 - Epoch [1/2], Step [91/160], Loss: 1.0834\n",
      "Phase 1 - Epoch [1/2], Step [92/160], Loss: 1.0450\n",
      "Phase 1 - Epoch [1/2], Step [93/160], Loss: 1.0893\n",
      "Phase 1 - Epoch [1/2], Step [94/160], Loss: 1.1140\n",
      "Phase 1 - Epoch [1/2], Step [95/160], Loss: 1.0105\n",
      "Phase 1 - Epoch [1/2], Step [96/160], Loss: 1.0257\n",
      "Phase 1 - Epoch [1/2], Step [97/160], Loss: 1.0875\n",
      "Phase 1 - Epoch [1/2], Step [98/160], Loss: 1.0677\n",
      "Phase 1 - Epoch [1/2], Step [99/160], Loss: 1.0017\n",
      "Phase 1 - Epoch [1/2], Step [100/160], Loss: 1.0748\n",
      "Phase 1 - Epoch [1/2], Step [101/160], Loss: 0.9880\n",
      "Phase 1 - Epoch [1/2], Step [102/160], Loss: 1.0538\n",
      "Phase 1 - Epoch [1/2], Step [103/160], Loss: 1.1157\n",
      "Phase 1 - Epoch [1/2], Step [104/160], Loss: 0.9954\n",
      "Phase 1 - Epoch [1/2], Step [105/160], Loss: 0.9812\n",
      "Phase 1 - Epoch [1/2], Step [106/160], Loss: 0.9702\n",
      "Phase 1 - Epoch [1/2], Step [107/160], Loss: 0.9665\n",
      "Phase 1 - Epoch [1/2], Step [108/160], Loss: 1.0736\n",
      "Phase 1 - Epoch [1/2], Step [109/160], Loss: 1.0192\n",
      "Phase 1 - Epoch [1/2], Step [110/160], Loss: 1.0601\n",
      "Phase 1 - Epoch [1/2], Step [111/160], Loss: 1.1355\n",
      "Phase 1 - Epoch [1/2], Step [112/160], Loss: 1.0610\n",
      "Phase 1 - Epoch [1/2], Step [113/160], Loss: 1.0638\n",
      "Phase 1 - Epoch [1/2], Step [114/160], Loss: 1.0493\n",
      "Phase 1 - Epoch [1/2], Step [115/160], Loss: 1.0342\n",
      "Phase 1 - Epoch [1/2], Step [116/160], Loss: 1.0589\n",
      "Phase 1 - Epoch [1/2], Step [117/160], Loss: 1.1067\n",
      "Phase 1 - Epoch [1/2], Step [118/160], Loss: 1.0823\n",
      "Phase 1 - Epoch [1/2], Step [119/160], Loss: 1.0696\n",
      "Phase 1 - Epoch [1/2], Step [120/160], Loss: 1.0862\n",
      "Phase 1 - Epoch [1/2], Step [121/160], Loss: 1.0922\n",
      "Phase 1 - Epoch [1/2], Step [122/160], Loss: 1.1166\n",
      "Phase 1 - Epoch [1/2], Step [123/160], Loss: 1.0304\n",
      "Phase 1 - Epoch [1/2], Step [124/160], Loss: 1.1617\n",
      "Phase 1 - Epoch [1/2], Step [125/160], Loss: 0.9561\n",
      "Phase 1 - Epoch [1/2], Step [126/160], Loss: 1.1059\n",
      "Phase 1 - Epoch [1/2], Step [127/160], Loss: 1.0484\n",
      "Phase 1 - Epoch [1/2], Step [128/160], Loss: 1.0619\n",
      "Phase 1 - Epoch [1/2], Step [129/160], Loss: 1.0958\n",
      "Phase 1 - Epoch [1/2], Step [130/160], Loss: 1.1156\n",
      "Phase 1 - Epoch [1/2], Step [131/160], Loss: 1.1110\n",
      "Phase 1 - Epoch [1/2], Step [132/160], Loss: 1.1061\n",
      "Phase 1 - Epoch [1/2], Step [133/160], Loss: 1.1297\n",
      "Phase 1 - Epoch [1/2], Step [134/160], Loss: 1.0255\n",
      "Phase 1 - Epoch [1/2], Step [135/160], Loss: 1.0660\n",
      "Phase 1 - Epoch [1/2], Step [136/160], Loss: 1.0880\n",
      "Phase 1 - Epoch [1/2], Step [137/160], Loss: 0.9836\n",
      "Phase 1 - Epoch [1/2], Step [138/160], Loss: 1.0794\n",
      "Phase 1 - Epoch [1/2], Step [139/160], Loss: 1.1074\n",
      "Phase 1 - Epoch [1/2], Step [140/160], Loss: 1.2343\n",
      "Phase 1 - Epoch [1/2], Step [141/160], Loss: 1.1909\n",
      "Phase 1 - Epoch [1/2], Step [142/160], Loss: 1.0364\n",
      "Phase 1 - Epoch [1/2], Step [143/160], Loss: 0.9803\n",
      "Phase 1 - Epoch [1/2], Step [144/160], Loss: 0.9966\n",
      "Phase 1 - Epoch [1/2], Step [145/160], Loss: 1.0432\n",
      "Phase 1 - Epoch [1/2], Step [146/160], Loss: 1.0221\n",
      "Phase 1 - Epoch [1/2], Step [147/160], Loss: 1.1232\n",
      "Phase 1 - Epoch [1/2], Step [148/160], Loss: 1.0489\n",
      "Phase 1 - Epoch [1/2], Step [149/160], Loss: 0.9870\n",
      "Phase 1 - Epoch [1/2], Step [150/160], Loss: 1.0666\n",
      "Phase 1 - Epoch [1/2], Step [151/160], Loss: 1.1119\n",
      "Phase 1 - Epoch [1/2], Step [152/160], Loss: 0.9740\n",
      "Phase 1 - Epoch [1/2], Step [153/160], Loss: 1.1292\n",
      "Phase 1 - Epoch [1/2], Step [154/160], Loss: 0.9696\n",
      "Phase 1 - Epoch [1/2], Step [155/160], Loss: 0.9703\n",
      "Phase 1 - Epoch [1/2], Step [156/160], Loss: 1.0398\n",
      "Phase 1 - Epoch [1/2], Step [157/160], Loss: 0.9667\n",
      "Phase 1 - Epoch [1/2], Step [158/160], Loss: 1.0680\n",
      "Phase 1 - Epoch [1/2], Step [159/160], Loss: 1.1440\n",
      "Phase 1 - Epoch [1/2], Step [160/160], Loss: 1.0164\n",
      "Validation Loss: 1.0311\n",
      "Train F1: 0.4170, Train Recall: 0.4181, Train Accuracy: 0.4181\n",
      "Val F1: 0.4796, Val Recall: 0.4793, Val Accuracy: 0.4793\n",
      "Best recall so far: 0.5777951524628616, Best parameters so far: {'dropout': 0.3, 'lr': 0.001}\n",
      "Phase 1 - Epoch [2/2], Step [1/160], Loss: 0.9758\n",
      "Phase 1 - Epoch [2/2], Step [2/160], Loss: 0.9089\n",
      "Phase 1 - Epoch [2/2], Step [3/160], Loss: 1.2081\n",
      "Phase 1 - Epoch [2/2], Step [4/160], Loss: 1.1289\n",
      "Phase 1 - Epoch [2/2], Step [5/160], Loss: 1.1306\n",
      "Phase 1 - Epoch [2/2], Step [6/160], Loss: 1.0193\n",
      "Phase 1 - Epoch [2/2], Step [7/160], Loss: 1.0188\n",
      "Phase 1 - Epoch [2/2], Step [8/160], Loss: 1.0851\n",
      "Phase 1 - Epoch [2/2], Step [9/160], Loss: 1.1754\n",
      "Phase 1 - Epoch [2/2], Step [10/160], Loss: 1.1887\n",
      "Phase 1 - Epoch [2/2], Step [11/160], Loss: 1.1395\n",
      "Phase 1 - Epoch [2/2], Step [12/160], Loss: 0.9708\n",
      "Phase 1 - Epoch [2/2], Step [13/160], Loss: 0.9667\n",
      "Phase 1 - Epoch [2/2], Step [14/160], Loss: 1.1608\n",
      "Phase 1 - Epoch [2/2], Step [15/160], Loss: 1.1434\n",
      "Phase 1 - Epoch [2/2], Step [16/160], Loss: 1.1790\n",
      "Phase 1 - Epoch [2/2], Step [17/160], Loss: 1.1198\n",
      "Phase 1 - Epoch [2/2], Step [18/160], Loss: 1.1069\n",
      "Phase 1 - Epoch [2/2], Step [19/160], Loss: 1.0317\n",
      "Phase 1 - Epoch [2/2], Step [20/160], Loss: 1.0118\n",
      "Phase 1 - Epoch [2/2], Step [21/160], Loss: 1.0855\n",
      "Phase 1 - Epoch [2/2], Step [22/160], Loss: 1.1197\n",
      "Phase 1 - Epoch [2/2], Step [23/160], Loss: 1.1223\n",
      "Phase 1 - Epoch [2/2], Step [24/160], Loss: 1.0733\n",
      "Phase 1 - Epoch [2/2], Step [25/160], Loss: 1.0591\n",
      "Phase 1 - Epoch [2/2], Step [26/160], Loss: 1.0351\n",
      "Phase 1 - Epoch [2/2], Step [27/160], Loss: 1.0627\n",
      "Phase 1 - Epoch [2/2], Step [28/160], Loss: 1.2092\n",
      "Phase 1 - Epoch [2/2], Step [29/160], Loss: 1.0118\n",
      "Phase 1 - Epoch [2/2], Step [30/160], Loss: 1.0292\n",
      "Phase 1 - Epoch [2/2], Step [31/160], Loss: 0.9780\n",
      "Phase 1 - Epoch [2/2], Step [32/160], Loss: 1.0809\n",
      "Phase 1 - Epoch [2/2], Step [33/160], Loss: 1.0818\n",
      "Phase 1 - Epoch [2/2], Step [34/160], Loss: 1.0755\n",
      "Phase 1 - Epoch [2/2], Step [35/160], Loss: 1.0384\n",
      "Phase 1 - Epoch [2/2], Step [36/160], Loss: 1.0053\n",
      "Phase 1 - Epoch [2/2], Step [37/160], Loss: 0.9572\n",
      "Phase 1 - Epoch [2/2], Step [38/160], Loss: 1.0073\n",
      "Phase 1 - Epoch [2/2], Step [39/160], Loss: 1.1295\n",
      "Phase 1 - Epoch [2/2], Step [40/160], Loss: 0.9132\n",
      "Phase 1 - Epoch [2/2], Step [41/160], Loss: 1.0088\n",
      "Phase 1 - Epoch [2/2], Step [42/160], Loss: 0.9943\n",
      "Phase 1 - Epoch [2/2], Step [43/160], Loss: 1.1650\n",
      "Phase 1 - Epoch [2/2], Step [44/160], Loss: 1.0775\n",
      "Phase 1 - Epoch [2/2], Step [45/160], Loss: 0.9205\n",
      "Phase 1 - Epoch [2/2], Step [46/160], Loss: 0.9233\n",
      "Phase 1 - Epoch [2/2], Step [47/160], Loss: 0.8846\n",
      "Phase 1 - Epoch [2/2], Step [48/160], Loss: 0.9694\n",
      "Phase 1 - Epoch [2/2], Step [49/160], Loss: 0.9101\n",
      "Phase 1 - Epoch [2/2], Step [50/160], Loss: 1.0907\n",
      "Phase 1 - Epoch [2/2], Step [51/160], Loss: 0.9798\n",
      "Phase 1 - Epoch [2/2], Step [52/160], Loss: 1.0703\n",
      "Phase 1 - Epoch [2/2], Step [53/160], Loss: 1.0338\n",
      "Phase 1 - Epoch [2/2], Step [54/160], Loss: 1.0280\n",
      "Phase 1 - Epoch [2/2], Step [55/160], Loss: 1.1826\n",
      "Phase 1 - Epoch [2/2], Step [56/160], Loss: 0.9707\n",
      "Phase 1 - Epoch [2/2], Step [57/160], Loss: 1.1729\n",
      "Phase 1 - Epoch [2/2], Step [58/160], Loss: 1.1036\n",
      "Phase 1 - Epoch [2/2], Step [59/160], Loss: 1.1512\n",
      "Phase 1 - Epoch [2/2], Step [60/160], Loss: 0.9156\n",
      "Phase 1 - Epoch [2/2], Step [61/160], Loss: 1.0100\n",
      "Phase 1 - Epoch [2/2], Step [62/160], Loss: 0.9872\n",
      "Phase 1 - Epoch [2/2], Step [63/160], Loss: 1.0959\n",
      "Phase 1 - Epoch [2/2], Step [64/160], Loss: 1.0420\n",
      "Phase 1 - Epoch [2/2], Step [65/160], Loss: 0.8764\n",
      "Phase 1 - Epoch [2/2], Step [66/160], Loss: 0.9802\n",
      "Phase 1 - Epoch [2/2], Step [67/160], Loss: 0.9585\n",
      "Phase 1 - Epoch [2/2], Step [68/160], Loss: 0.8776\n",
      "Phase 1 - Epoch [2/2], Step [69/160], Loss: 0.9186\n",
      "Phase 1 - Epoch [2/2], Step [70/160], Loss: 0.9650\n",
      "Phase 1 - Epoch [2/2], Step [71/160], Loss: 0.9071\n",
      "Phase 1 - Epoch [2/2], Step [72/160], Loss: 0.9723\n",
      "Phase 1 - Epoch [2/2], Step [73/160], Loss: 1.0025\n",
      "Phase 1 - Epoch [2/2], Step [74/160], Loss: 0.8857\n",
      "Phase 1 - Epoch [2/2], Step [75/160], Loss: 1.1464\n",
      "Phase 1 - Epoch [2/2], Step [76/160], Loss: 1.0225\n",
      "Phase 1 - Epoch [2/2], Step [77/160], Loss: 0.9843\n",
      "Phase 1 - Epoch [2/2], Step [78/160], Loss: 0.9841\n",
      "Phase 1 - Epoch [2/2], Step [79/160], Loss: 0.8592\n",
      "Phase 1 - Epoch [2/2], Step [80/160], Loss: 1.0217\n",
      "Phase 1 - Epoch [2/2], Step [81/160], Loss: 0.9358\n",
      "Phase 1 - Epoch [2/2], Step [82/160], Loss: 1.0604\n",
      "Phase 1 - Epoch [2/2], Step [83/160], Loss: 1.1131\n",
      "Phase 1 - Epoch [2/2], Step [84/160], Loss: 0.9686\n",
      "Phase 1 - Epoch [2/2], Step [85/160], Loss: 1.0529\n",
      "Phase 1 - Epoch [2/2], Step [86/160], Loss: 1.0633\n",
      "Phase 1 - Epoch [2/2], Step [87/160], Loss: 1.2209\n",
      "Phase 1 - Epoch [2/2], Step [88/160], Loss: 1.0069\n",
      "Phase 1 - Epoch [2/2], Step [89/160], Loss: 0.9517\n",
      "Phase 1 - Epoch [2/2], Step [90/160], Loss: 0.9458\n",
      "Phase 1 - Epoch [2/2], Step [91/160], Loss: 1.0145\n",
      "Phase 1 - Epoch [2/2], Step [92/160], Loss: 0.9471\n",
      "Phase 1 - Epoch [2/2], Step [93/160], Loss: 1.1609\n",
      "Phase 1 - Epoch [2/2], Step [94/160], Loss: 1.1633\n",
      "Phase 1 - Epoch [2/2], Step [95/160], Loss: 1.0473\n",
      "Phase 1 - Epoch [2/2], Step [96/160], Loss: 1.0845\n",
      "Phase 1 - Epoch [2/2], Step [97/160], Loss: 1.0871\n",
      "Phase 1 - Epoch [2/2], Step [98/160], Loss: 1.1282\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {'lr': [1e-3, 5e-4], 'dropout': [0.3, 0.4]}\n",
    "best_recall = 0\n",
    "best_params = {}\n",
    "\n",
    "# Grid search\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Training with parameters: {params}\")\n",
    "\n",
    "    # Initialize metrics storage\n",
    "    train_f1, train_recall, train_accuracy = [], [], []\n",
    "    val_f1, val_recall, val_accuracy = [], [], []\n",
    "\n",
    "    # Initialize model and optimizer here, otherwise they will carry state\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(num_ftrs, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(params['dropout']),\n",
    "        nn.Linear(256, 3)\n",
    "    )\n",
    "\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.fc.parameters(), lr=params['lr'])\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience_limit = 3\n",
    "\n",
    "    # Training Loop - Phase 1 (Train only the new layer)\n",
    "    for epoch in range(15):\n",
    "        model.train()\n",
    "        true_labels, pred_labels = [], []\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            pred_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "            print(f\"Phase 1 - Epoch [{epoch+1}/2], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Metrics for training\n",
    "        train_f1.append(f1_score(true_labels, pred_labels, average='weighted'))\n",
    "        train_recall.append(recall_score(true_labels, pred_labels, average='weighted'))\n",
    "        train_accuracy.append(accuracy_score(true_labels, pred_labels))\n",
    "\n",
    "        # Validation Loop and metrics\n",
    "        model.eval()\n",
    "        true_labels, pred_labels = [], []\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "                pred_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_f1.append(f1_score(true_labels, pred_labels, average='weighted'))\n",
    "        val_recall.append(recall_score(true_labels, pred_labels, average='weighted'))\n",
    "        val_accuracy.append(accuracy_score(true_labels, pred_labels))\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Train F1: {train_f1[-1]:.4f}, Train Recall: {train_recall[-1]:.4f}, Train Accuracy: {train_accuracy[-1]:.4f}\")\n",
    "        print(f\"Val F1: {val_f1[-1]:.4f}, Val Recall: {val_recall[-1]:.4f}, Val Accuracy: {val_accuracy[-1]:.4f}\")\n",
    "        \n",
    "        current_val_recall = val_recall[-1]\n",
    "\n",
    "        if current_val_recall > best_recall:\n",
    "            best_recall = current_val_recall\n",
    "            best_params = params\n",
    "\n",
    "        print(f\"Best recall so far: {best_recall}, Best parameters so far: {best_params}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience_limit:\n",
    "            print(\"Early stopping in Phase 1.\")\n",
    "            break\n",
    "\n",
    "        # Phase 2: Unfreeze some layers (here layer4) and fine-tune\n",
    "        for param in model.layer4.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Update optimizer for Phase 2\n",
    "        optimizer = optim.Adam([\n",
    "            {'params': model.layer4.parameters(), 'lr': 1e-4},\n",
    "            {'params': model.fc.parameters(), 'lr': 1e-3}\n",
    "        ])\n",
    "\n",
    "        # Training Loop - Phase 2\n",
    "    for epoch in range(15):\n",
    "        model.train()\n",
    "        true_labels, pred_labels = [], []\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            pred_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "            print(f\"Phase 2 - Epoch [{epoch+1}/2], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Metrics for training\n",
    "        train_f1.append(f1_score(true_labels, pred_labels, average='weighted'))\n",
    "        train_recall.append(recall_score(true_labels, pred_labels, average='weighted'))\n",
    "        train_accuracy.append(accuracy_score(true_labels, pred_labels))\n",
    "\n",
    "        # Validation Loop and metrics\n",
    "        model.eval()\n",
    "        true_labels, pred_labels = [], []\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "                pred_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_f1.append(f1_score(true_labels, pred_labels, average='weighted'))\n",
    "        val_recall.append(recall_score(true_labels, pred_labels, average='weighted'))\n",
    "        val_accuracy.append(accuracy_score(true_labels, pred_labels))\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Train F1: {train_f1[-1]:.4f}, Train Recall: {train_recall[-1]:.4f}, Train Accuracy: {train_accuracy[-1]:.4f}\")\n",
    "        print(f\"Val F1: {val_f1[-1]:.4f}, Val Recall: {val_recall[-1]:.4f}, Val Accuracy: {val_accuracy[-1]:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience_limit:\n",
    "            print(\"Early stopping in Phase 2.\")\n",
    "            break\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    print(f\"Best recall so far: {best_recall}, Best parameters so far: {best_params}\")\n",
    "\n",
    "print(f\"Best parameters found: {best_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
